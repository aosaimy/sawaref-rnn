{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1. Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "format": "column",
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "'''\n",
    "    An implementation of sequence to sequence learning\n",
    "    for performing ensemble morphosyntactic analyses\n",
    "'''\n",
    "from __future__ import print_function\n",
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "from six.moves import range\n",
    "from prepare_data import SawarefData, padIndexes\n",
    "from character_table import colors, CharacterTable, eprint\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import re\n",
    "import pickle\n",
    "import sys\n",
    "import datetime\n",
    "from keras.callbacks import TensorBoard\n",
    "from buckwalter import utf2bw, bw2utf\n",
    "from pprint import pprint\n",
    "\n",
    "# do not import in interactive mode\n",
    "# from vis import SawarefVis\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras import layers\n",
    "from keras.callbacks import EarlyStopping, Callback\n",
    "from keras.utils import plot_model\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "emb = FastText.load_fasttext_format(\"/Users/abbander/Leeds/OpenArabic/data/classical.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# emb.wv.most_similar(\"النار\", topn=100)\n",
    "emb.vector_size\n",
    "print(\"asd\",flush=T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "collapsed": false,
    "deletable": false,
    "editable": false,
    "format": "column",
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 2. Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "MYPATH = \"/morpho/output/\"\n",
    "# Parameters for the model and dataset.\n",
    "TRAINING_SIZE = 50000\n",
    "EPOCHS = 40\n",
    "EMBEDDINGS = 100\n",
    "# DIGITS = 3\n",
    "# REVERSE = True\n",
    "# Try replacing GRU, or SimpleRNN.\n",
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 64\n",
    "LAYERS = 1\n",
    "ITERATIONS = 10\n",
    "REVERSE = False\n",
    "MODEL_NAME = \"main-seq-multiinput-multioutput-segmentation.keras\"\n",
    "DATA_PICKLE = \"main-seq-multiinput-multioutput-segmentation.pickle\"\n",
    "RNN = layers.LSTM\n",
    "CATS_EMBEDDING = 5\n",
    "TEST_SPLIT = 1  #  means 0.1 of data is for test\n",
    "VAL_SPLIT = 1  #  means 0.1 of data is for test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 3. Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "`feat_x` is the input categorical features\n",
    "\n",
    "`feat_y` is the output categorical features\n",
    "\n",
    "`strings_x` is the input character-based strings features\n",
    "\n",
    "`strings_y` is the output character-based strings features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tools = [\"MX\", \"FA\", \"AM\", \"ST\"]\n",
    "import itertools\n",
    "\n",
    "for L in range(1, len(tools)):\n",
    "    for subset in itertools.combinations(tools, L):\n",
    "        print(subset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "feat_x = [\n",
    "    \"MXpos\", \"STpos\", \"AMpos\", \"FApos\", \"STaspect\", \"AMaspect\", \"MXaspect\",\n",
    "    \"FAaspect\", \"STperson\", \"AMperson\", \"MXperson\", \"FAperson\", \"STgender\",\n",
    "    \"AMgender\", \"MXgender\", \"FAgender\", \"STnumber\", \"AMnumber\", \"MXnumber\",\n",
    "    \"FAnumber\", \"STcase\", \"AMcase\", \"MXcase\", \"FAcase\", \"STvoice\", \"AMvoice\",\n",
    "    \"MXvoice\", \"FAvoice\", \"STmood\", \"AMmood\", \"MXmood\", \"FAmood\", \"STstate\",\n",
    "    \"AMstate\", \"MXstate\", \"FAstate\"\n",
    "]\n",
    "feat_y = [\n",
    "    \"QApos\", \"QAaspect\", \"QAperson\", \"QAgender\", \"QAnumber\", \"QAcase\",\n",
    "    \"QAvoice\", \"QAmood\", \"QAstate\"\n",
    "]\n",
    "strings_x = [\"QAwutf8\"]\n",
    "strings_y = [\"QAutf8\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def pretty_join(arr):\n",
    "    if isinstance(arr, pd.Series):\n",
    "        arr = arr.to_frame().T\n",
    "    if isinstance(arr.columns, pd.core.index.MultiIndex):\n",
    "        return \"/\".join([\n",
    "            '+'.join([\n",
    "                x[1] for x in arr.columns[row == 1]\n",
    "                if x[1][-2:] != \"na\" and x[1][-2:] != \"_0\"\n",
    "            ]) for index, row in arr.iterrows()\n",
    "        ])\n",
    "    else:\n",
    "        return \"/\".join([\n",
    "            '+'.join([\n",
    "                x for x in arr.columns[row == 1]\n",
    "                if x[-2:] != \"na\" and x[-2:] != \"_0\"\n",
    "            ]) for index, row in arr.iterrows()\n",
    "        ])\n",
    "\n",
    "\n",
    "def pretty_value(colum_value):\n",
    "    return re.sub(\".*_\", \"\", colum_value)\n",
    "\n",
    "\n",
    "def getValuesAndReshape(df, middle_dim):\n",
    "    return df.values.reshape((df.shape[0] // middle_dim, middle_dim, -1))\n",
    "\n",
    "\n",
    "#     return df.values.reshape((df.shape[0]//middle_dim, middle_dim, df.shape[1]))\n",
    "\n",
    "\n",
    "def flattencolumns(df1, cols):\n",
    "    df = pd.concat(\n",
    "        [pd.DataFrame(df1[x].values.tolist()).add_prefix(x) for x in cols],\n",
    "        axis=1)\n",
    "    return pd.concat([df, df1.drop(cols, axis=1)], axis=1)\n",
    "\n",
    "\n",
    "def truncate(x):\n",
    "    return x[:EMBEDDINGS]\n",
    "\n",
    "\n",
    "def removeDiac(x):\n",
    "    return re.sub('[ًٌٍَُِّْ]', '', x.replace(\"ٱ\",\"ا\"))\n",
    "\n",
    "\n",
    "def padStringWithSpaces(x):\n",
    "    return x + ' ' * (STRING_LENGTH - len(x))\n",
    "\n",
    "\n",
    "def joinMorphemesStrings(arr):\n",
    "    return \"+\".join([\n",
    "        x for x in arr if isinstance(x, float) == False and len(x) != 0\n",
    "        and x != \"-----\" and x != \"-\"\n",
    "    ])\n",
    "\n",
    "\n",
    "def fullprint(*args, **kwargs):\n",
    "    opt = np.get_printoptions()\n",
    "    np.set_printoptions(threshold='nan')\n",
    "    pprint(*args, **kwargs)\n",
    "    np.set_printoptions(**opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 4. Loading data from sawaref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sawarefData = SawarefData(\n",
    "    MYPATH,\n",
    "    EMBEDDINGS,\n",
    "    feat_x=feat_x,\n",
    "    strings_x=strings_x,\n",
    "    strings_y=strings_y,\n",
    "    feat_y=feat_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "source = list(itertools.chain(*sawarefData.quran_sent))\n",
    "df = pd.DataFrame(\n",
    "    source,\n",
    "    columns=[\"sid\", \"aid\", \"wid\", \"mid\"] + feat_x + strings_x + strings_y +\n",
    "    [\"embeddings\"] + feat_y)\n",
    "df[\"embeddings\"] = df[\"embeddings\"].apply(truncate)\n",
    "df = flattencolumns(df, [\"embeddings\"])\n",
    "df.set_index([\"sid\", \"aid\", \"wid\", \"mid\"], inplace=True)\n",
    "df.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## 2. Pad the rows according to the longest word (in # of morphemes)\n",
    "SENTLEN = max(df.index.get_level_values(\"mid\"))\n",
    "df = df.reindex(\n",
    "    padIndexes(df, max(df.index.get_level_values(\"mid\"))),\n",
    "    fill_value=0).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## 3. Get the hot encoding of all caterogirical data (see columns attr)\n",
    "dumm = pd.get_dummies(df, columns=feat_x + feat_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## 4. Add two-level columns for easy indexing later (wid, mid)\n",
    "EXAMPLES_LEN = df.shape[0] // SENTLEN\n",
    "new_columns = []\n",
    "for x in dumm.columns:\n",
    "    new_columns.append(re.sub('(_.*|[0-9]*)', '', x))\n",
    "dumm.columns = [new_columns, dumm.columns]\n",
    "dumm.index = [[x for x in range(EXAMPLES_LEN) for _ in range(SENTLEN)],\n",
    "              [x for _ in range(EXAMPLES_LEN) for x in range(SENTLEN)]]\n",
    "dumm.sort_index(axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## 5. Find possible values of each cat\n",
    "def getSet(df):\n",
    "    results = set()\n",
    "    df.apply(results.add)\n",
    "    return results\n",
    "\n",
    "\n",
    "embeddingInputSets = {i: getSet(df[i]) for i in feat_x}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "collapsed": false,
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "embeddingInputSets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "feat_x = list(set(feat_x) - set([x for x in feat_x if len( embeddingInputSets[x])<=1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df2 = pd.concat([df.reset_index(), dumm.reset_index()], axis=1)\n",
    "# df2.set_index([\"sid\", \"aid\", \"wid\", \"mid\"], inplace=True)\n",
    "df2.index = [[x for x in range(EXAMPLES_LEN) for _ in range(SENTLEN)],\n",
    "             [x for _ in range(EXAMPLES_LEN) for x in range(SENTLEN)]]\n",
    "\n",
    "df2.drop(['embeddings' + str(x) for x in range(100)], inplace=True, axis=1)\n",
    "# df2.columns = [(x,\"val\") if isinstance(x,str) else x  for x in df2.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df2.index = [[x for x in range(EXAMPLES_LEN) for _ in range(SENTLEN)],\n",
    "             [x for _ in range(EXAMPLES_LEN) for x in range(SENTLEN)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df2.columns = [[\"vals\" if isinstance(x, str) else x[0] for x in df2.columns],\n",
    "               [x if isinstance(x, str) else x[1] for x in df2.columns]]\n",
    "\n",
    "df2.sort_index(axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "del df\n",
    "del dumm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## 5. Prepare string columns:\n",
    "# a. clean all padded rows\n",
    "strings = strings_x + strings_y\n",
    "for s in strings:\n",
    "    #         print(df2[s])\n",
    "    df2.loc[df2[(\"vals\", s)] == 0, (\"vals\", s)] = \"\"\n",
    "    df2.loc[pd.isna(df2[(\"vals\", s)]), (\"vals\", s)] = \"\"\n",
    "for x in strings:\n",
    "    df2[(x, \"undiac\")] = df2[(\"vals\", x)].apply(removeDiac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "df2[\"vals\", \"QAutf8\"].groupby(level=[0]).apply(joinMorphemesStrings).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# b. group them by morpheme and join with \"+\"\"\n",
    "df_strings = pd.DataFrame({\n",
    "    x: df2[\"vals\", x].groupby(level=[0]).apply(joinMorphemesStrings)\n",
    "    for x in strings\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "df_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# c. pad joined morphemes\n",
    "STRING_LENGTH = max([len(x) for k in strings for x in df_strings[k]])\n",
    "for s in strings:\n",
    "    df_strings[s] = df_strings[s].apply(padStringWithSpaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# d. encode them in one hot encoding\n",
    "charset = set(\"+\").union(\n",
    "    *[list(set(\"\".join(df_strings[x] + \"-\"))) for x in strings])\n",
    "ctable = CharacterTable(charset, STRING_LENGTH)\n",
    "### Now we have one shape for all strings: (STRING_LENGTH, len(charset))\n",
    "for x in strings:\n",
    "    df_strings[x + \"_onehot\"] = df_strings[x].apply(ctable.encode)\n",
    "df_strings['num'] = [x for x in range(len(df_strings))]\n",
    "df_strings.set_index('num', append=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# e. remove diac\n",
    "for x in strings:\n",
    "    df_strings[x+ \"_undiac\"] = df_strings[x].apply(removeDiac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# f. encode them as dense vector using fastText\n",
    "def emb_encode(x):\n",
    "    return np.zeros(emb.vector_size) if x.strip()==\"\" else emb[x]\n",
    "for x in strings:\n",
    "    df_strings[x + \"_emb\"] = df_strings[x+\"_undiac\"].apply(emb_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_strings[\"QAutf8_emb\"][1]\n",
    "df_strings[1:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 5. Save (or load) the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "pickle.dump([df2, df_strings], open(DATA_PICKLE, mode=\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "arr = pickle.load(open(DATA_PICKLE, mode=\"rb\"), encoding=\"UTF8\")\n",
    "(dumm, df_strings) = arr[0], arr[1]\n",
    "SENTLEN = max(dumm.index.get_level_values(1)) + 1\n",
    "EXAMPLES_LEN = dumm.shape[0] // SENTLEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 6. Prepare splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# 6. Shuffle (x, y) in unison\n",
    "indices = list(range(EXAMPLES_LEN))\n",
    "np.random.shuffle(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# 7. Explicitly set apart 10% for validation data that we never train over.\n",
    "test_split_at = int(EXAMPLES_LEN * TEST_SPLIT / 10)\n",
    "val_split_at = int(EXAMPLES_LEN * TEST_SPLIT / 10 +\n",
    "                   EXAMPLES_LEN * VAL_SPLIT / 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "print(EXAMPLES_LEN, val_split_at, test_split_at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def getMorphemeBasedIndeicies(arr):\n",
    "    return np.array([list(range(x*5,x*5+5)) for x in arr]).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "getMorphemeBasedIndeicies(indices[:test_split_at])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "values_test = df_strings.iloc[indices[:test_split_at]]\n",
    "values_val = df_strings.iloc[indices[test_split_at:val_split_at]]\n",
    "values_train = df_strings.iloc[indices[val_split_at:]]\n",
    "\n",
    "test = df2.iloc[getMorphemeBasedIndeicies(indices[:test_split_at])]\n",
    "val = df2.iloc[getMorphemeBasedIndeicies(indices[test_split_at:val_split_at])]\n",
    "train = df2.iloc[getMorphemeBasedIndeicies(indices[val_split_at:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "train[\"embeddings\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "rand=np.random.randint(0,len(values_val))\n",
    "print(rand, values_test.iloc[rand])\n",
    "test[\"QApos\"].iloc[rand*5:rand*5+5].idxmax(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "print(values_test.shape, values_val.shape, values_train.shape)\n",
    "print(test.shape, val.shape, train.shape)\n",
    "print(test.shape[0]//5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "TEST = np.random.randint(0, len(values_train.index))\n",
    "print(TEST)\n",
    "print(values_train.loc[values_train.index[TEST], [\"QAutf8\"]])\n",
    "print(train.loc[train.index[TEST], \"QAutf8\"])\n",
    "\n",
    "# TEST = np.random.randint(0, len(train.index))\n",
    "# TEST = train.index[TEST]\n",
    "# print(values_train.loc[TEST,[\"QAutf8\"]])\n",
    "\n",
    "# train.loc[TEST,:]\n",
    "\n",
    "# arr = train.loc[TEST,:]\n",
    "# arr.to_frame().T\n",
    "# arr.index[arr == 1]\n",
    "pretty_join(train.loc[train.index[TEST], :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "len(values_train) * 5, len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "embeddingInputLists = {\n",
    "    i: [i + \"_\" + str(x) for x in embeddingInputSets[i]]\n",
    "    for i in feat_x\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def getData(cat_source, str_source, cats_feats=[], strs_feats=[], embeddings=False):\n",
    "    data = {\n",
    "        **{i:\n",
    "           getValuesAndReshape(cat_source[i].idxmax(axis=1).apply(lambda x: embeddingInputLists[i].index(x)),SENTLEN)\n",
    "#            getValuesAndReshape(cat_source.loc[:, (\"vals\", i)], SENTLEN)\n",
    "               if i in embeddingInputSets and len(embeddingInputSets[i]) > CATS_EMBEDDING \n",
    "               else getValuesAndReshape(cat_source[i], SENTLEN) \n",
    "           for i in cats_feats},\n",
    "        **{i:np.stack(str_source[i+\"_onehot\"].values) for i in strs_feats},\n",
    "    }\n",
    "    if embeddings:\n",
    "        for i in strs_feats:\n",
    "            data[i+\"_emb\"] = np.stack(str_source[i+\"_emb\"].values)\n",
    "    return data\n",
    "\n",
    "data = {\n",
    "    'input': getData(train, values_train, cats_feats=feat_x, strs_feats=strings_x, embeddings=True),\n",
    "    'output': getData(train, values_train, cats_feats=feat_y, strs_feats=strings_y),\n",
    "    'val': (\n",
    "        getData(val, values_val, cats_feats=feat_x, strs_feats=strings_x, embeddings=True),\n",
    "        getData(val, values_val, cats_feats=feat_y, strs_feats=strings_y)\n",
    "    ),\n",
    "    'test': (\n",
    "        getData(test, values_test, cats_feats=feat_x, strs_feats=strings_x, embeddings=True),\n",
    "        getData(test, values_test, cats_feats=feat_y, strs_feats=strings_y)\n",
    "    )\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "data[\"input\"][\"QAwutf8_emb\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# 8. Some info about shapes\n",
    "print('\\nTraining Data:')\n",
    "print(\"\\n\".join([\"X: \"+i+str(data[\"input\"][i].shape) for i in feat_x]))\n",
    "print(\"\\n\".join([\"Y: \"+i+str(data[\"output\"][i].shape) for i in feat_y]))\n",
    "print('\\nValidation Data:')\n",
    "print(\"\\n\".join([\"X: \"+i+str(data[\"val\"][0][i].shape) for i in feat_x]))\n",
    "print(\"\\n\".join([\"Y: \"+i+str(data[\"val\"][1][i].shape) for i in feat_y]))\n",
    "print('\\nTest Data:')\n",
    "print(\"\\n\".join([\"X: \"+i+str(data[\"test\"][0][i].shape) for i in feat_x]))\n",
    "print(\"\\n\".join([\"Y: \"+i+str(data[\"test\"][1][i].shape) for i in feat_y]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 7. Load Previous model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "model = load_model(MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 8. Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# print('Build model...')\n",
    "outputs = []\n",
    "inputs = []\n",
    "# For strings\n",
    "# strings_input = layers.Input(shape=(STRING_LENGTH, len(charset)), name=strings_x[0])\n",
    "# lstm_strings_encoder = layers.Bidirectional(RNN(HIDDEN_SIZE,name=\"lstm_strings_encoder\"))(strings_input)\n",
    "\n",
    "# For categoricals\n",
    "for i in feat_x:\n",
    "    inputs.append(layers.Input(shape=(SENTLEN, data[\"input\"][i].shape[2]), name=i))\n",
    "\n",
    "def getEmbedding(input):\n",
    "    name = input.name.split(\"_\")[0]\n",
    "    if name in feat_x and len(embeddingInputSets[name]) > CATS_EMBEDDING:\n",
    "        return layers.Reshape((SENTLEN, -1))(layers.Embedding(len(embeddingInputSets[name]),2, input_length=SENTLEN)(input))\n",
    "#         return layers.Dropout(0.1)(layers.Embedding(len(embeddingInputSets[name]),2)(input))\n",
    "    else:\n",
    "        return layers.Dropout(0.1)(input)\n",
    "    \n",
    "# main_input = layers.Concatenate()([layers.Dropout(0.1)(input) for input in inputs])\n",
    "main_input = layers.Concatenate()([getEmbedding(input) for input in inputs])\n",
    "# inputs.append(strings_input)\n",
    "\n",
    "lstm_out = layers.Bidirectional(RNN(HIDDEN_SIZE))(main_input)\n",
    "# input_shape=(None, len(ctable_x.chars) + EMBEDDINGS)))\n",
    "# As the decoder RNN's input, repeatedly provide with the last hidden state of\n",
    "# RNN for each time step. Repeat 'DIGITS + 1' times as that's the maximum\n",
    "# length of output, e.g., when DIGITS=3, max output is 999+999=1998.\n",
    "emb_input = layers.Input(shape=(emb.vector_size,), name=strings_x[0]+\"_emb\")\n",
    "inputs.append(emb_input)\n",
    "concatenated = layers.Concatenate()([lstm_out,emb_input])\n",
    "# concatenated = layers.Concatenate()([lstm_strings_encoder,lstm_out,emb_input])\n",
    "\n",
    "# For strings again\n",
    "repeat_strings_out = layers.RepeatVector(STRING_LENGTH)(concatenated)\n",
    "rnn_out = RNN(HIDDEN_SIZE, return_sequences=True)(repeat_strings_out)\n",
    "strings_output = layers.TimeDistributed(\n",
    "      layers.Dense(\n",
    "        len(charset), \n",
    "        activation=\"softmax\"), name=strings_y[0])(rnn_out)\n",
    "outputs.append(strings_output)\n",
    "\n",
    "dropout_out = layers.Dropout(0.5)(concatenated)\n",
    "repeat_out = layers.RepeatVector(SENTLEN)(dropout_out)\n",
    "# The decoder RNN could be multiple layers stacked or a single layer.\n",
    "rnn_out = RNN(HIDDEN_SIZE, return_sequences=True)(repeat_out)\n",
    "for _ in range(LAYERS-1):\n",
    "    # By setting return_sequences to True, return not only the last output but\n",
    "    # all the outputs so far in the form of (num_samples, timesteps,\n",
    "    # output_dim). This is necessary as TimeDistributed in the below expects\n",
    "    # the first dimension to be the timesteps.\n",
    "    rnn_out = RNN(HIDDEN_SIZE, return_sequences=True)(rnn_out)\n",
    "\n",
    "    \n",
    "\n",
    "# Apply a dense layer to the every temporal slice of an input. For each of step\n",
    "# of the output sequence, decide which character should be chosen.\n",
    "\n",
    "for i in feat_y:\n",
    "    outputs.append(\n",
    "      layers.TimeDistributed(\n",
    "      layers.Dense(\n",
    "        data[\"output\"][i].shape[2], \n",
    "        activation=\"softmax\"), name=i)(rnn_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "print(inputs,outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 9. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "format": "column"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "plot_model(model, to_file='model_only_emb.png', show_shapes=True)\n",
    "SVG(model_to_dot(model, show_shapes=True, show_layer_names=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 10. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "earlyStopping=EarlyStopping(monitor='val_loss', patience=5, verbose=0, mode='auto')\n",
    "\n",
    "tensorboard = TensorBoard(log_dir=\"logs/only_emb_+1_{}\".format(\n",
    "    datetime.datetime.now().strftime(\"%Y.%m.%d.%H.%M\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "[x for x in model.metrics_names if x[-4:]=='_acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class TestCallback(Callback):\n",
    "    def __init__(self, test_data):\n",
    "        self.test_data = test_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        x, y = self.test_data\n",
    "        metrics = self.model.evaluate(x, y, verbose=0)\n",
    "        for i,x in enumerate(self.model.metrics_names):\n",
    "            logs[\"test_\"+x]= metrics[i]\n",
    "        logs['test_acc']= np.mean([logs[\"test_\"+x] for x in self.model.metrics_names if x[-4:]=='_acc'])\n",
    "        logs['val_acc']= np.mean([logs[\"val_\"+x] for x in self.model.metrics_names if x[-4:]=='_acc'])\n",
    "        logs['train_acc']= np.mean([logs[x] for x in self.model.metrics_names if x[-4:]=='_acc'])\n",
    "        return logs\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "\"Not Working!!!!\"\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "class PrecRecMetrics(Callback):\n",
    "    def __init__(self, test_data=None):\n",
    "        self.test_data = test_data\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "\n",
    "#         self.compute((self.validation_data[0],self.validation_data[1]), \"val_\")\n",
    "        if self.test_data is not None:\n",
    "            self.compute(self.test_data, \"test_\", logs)            \n",
    "\n",
    "        return logs\n",
    "\n",
    "    def compute(self, data, prefix=\"na_\", logs={}):\n",
    "        global _\n",
    "        x, y = data\n",
    "        \n",
    "        outputs = self.model.predict(x)\n",
    "        _ = (data,outputs)\n",
    "        for i, o in enumerate(self.model.output_names):\n",
    "            y_predict = np.argmax(outputs[i], axis=-1)\n",
    "            yy = np.argmax(y[o], axis=-1)\n",
    "            logs[prefix+'recall'] = recall_score(y, y_predict)\n",
    "            logs[prefix+'precision'] =  precision_score(y, y_predict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(data['input'], data['output'],\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    callbacks=[earlyStopping, TestCallback(data['test']), tensorboard],\n",
    "                    epochs=100,\n",
    "                    verbose=2,\n",
    "                    validation_data=data['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "model.metrics_names\n",
    "model.output_names\n",
    "model.metrics\n",
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.plot(history.history['test_loss'])\n",
    "plt.title('model overall loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val', 'test'], loc='upper right')\n",
    "plt.savefig('plots/model_overall_loss' +\n",
    "            datetime.datetime.now().strftime(\".%Y.%m.%d.%H.%M\") + '.png')\n",
    "plt.close(fig)\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# In[471]:\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history[\"train_acc\"])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.plot(history.history['test_acc'])\n",
    "plt.title('model average accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend([x +\" = \"+ str(round(history.history[x+'_acc'][-1]*100,2)) for x in ['train', 'val', 'test']], loc='lower right')\n",
    "# for i, x in enumerate(['train', 'val', 'test']):\n",
    "#     plt.annotate(str(round(history.history[x+'_acc'][-1]*100,2)),\n",
    "#                  xy=(len(history.history[x+'_acc']), history.history[x+'_acc'][-1]), \n",
    "#                  textcoords='figure pixels', \n",
    "#                  xytext=(-20,-10))\n",
    "plt.savefig('plots/model_average_accuracy' +\n",
    "            datetime.datetime.now().strftime(\".%Y.%m.%d.%H.%M\") + '.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.plot(history.history['test_loss'])\n",
    "plt.title('model overall loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val', 'test'], loc='upper right')\n",
    "plt.savefig('plots/model_overall_loss' +\n",
    "            datetime.datetime.now().strftime(\".%Y.%m.%d.%H.%M\") + '.png')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "plt.plot(history.history[\"train_acc\"])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.plot(history.history['test_acc'])\n",
    "plt.title('model average accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend([x +\" = \"+ str(round(history.history[x+'_acc'][-1]*100,2)) for x in ['train', 'val', 'test']], loc='lower right')\n",
    "# for i, x in enumerate(['train', 'val', 'test']):\n",
    "#     plt.annotate(str(round(history.history[x+'_acc'][-1]*100,2)),\n",
    "#                  xy=(len(history.history[x+'_acc']), history.history[x+'_acc'][-1]), \n",
    "#                  textcoords='figure pixels', \n",
    "#                  xytext=(-20,-10))\n",
    "plt.savefig('plots/model_average_accuracy' +\n",
    "            datetime.datetime.now().strftime(\".%Y.%m.%d.%H.%M\") + '.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# VALIDATAION = True\n",
    "# prefix = \"val_\" if VALIDATAION else \"\"\n",
    "# del prefix\n",
    "legends = []\n",
    "for x in model.output_names:\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(history.history[x+\"_acc\"])\n",
    "    legends.append(\"\"+x +\" = \"+ str(round(history.history[x+'_acc'][-1]*100,2)))\n",
    "#     plt.plot(history.history[x+\"_acc\"])\n",
    "#     legends.append(\"val_\"+x)\n",
    "#     plt.plot(history.history[\"val_\" + x+\"_acc\"])\n",
    "#     legends.append(\"train_\"+x)\n",
    "plt.title('model indiviual accuracy on test dataset')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(legends, loc='lower right')\n",
    "plt.savefig('plots/accuracy_all' +\n",
    "            datetime.datetime.now().strftime(\".%Y.%m.%d.%H.%M\") + '.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "legends = []\n",
    "for x in model.output_names:\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history[\"test_\"+x+\"_loss\"])\n",
    "    legends.append(\"\"+x +\" = \"+ str(round(history.history[x+'_loss'][-1]*100,2)))\n",
    "plt.title('model individual loss on test dataset')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(legends, loc='upper right')\n",
    "plt.savefig('plots/loss_all' +\n",
    "            datetime.datetime.now().strftime(\".%Y.%m.%d.%H.%M\") + '.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 10.1 Inspect One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#%%capture --no-stderr cap\n",
    "%autoreload 2\n",
    "\n",
    "colors.ok = ''\n",
    "colors.fail = ''\n",
    "colors.close = ''\n",
    "\n",
    "\n",
    "def inspectOne(times=0, printCorrect=True):\n",
    "    isAllCorrect = True\n",
    "    if times > 10000: # to prevent infinite loops\n",
    "        return\n",
    "    ind = np.random.randint(0, len(val.index))\n",
    "    ind = (val.index[ind][0], slice(None))\n",
    "\n",
    "    string_input = values_test.loc[(slice(None), ind[0]), :]\n",
    "    preds = model.predict(\n",
    "        getData(val.loc[ind], string_input, feat_x, strings_x))\n",
    "    preds = [np.argmax(x, axis=-1) for x in preds]\n",
    "    #predicted string\n",
    "    print(\"Predicted String Q\", string_input[strings_x[0]][0], \"from\",\n",
    "          \"-\".join(str(x) for x in string_input.index.values[0]))\n",
    "\n",
    "    if (np.argmax(string_input[strings_y[0]][0], axis=-1) == preds[0]).all():\n",
    "        if printCorrect: print(colors.ok + '✅' + colors.close + \"Segmentation\")\n",
    "    else:\n",
    "        print(colors.fail + '❌' + colors.close + \"Segmentation\")\n",
    "        isAllCorrect = False\n",
    "        print(\"    T\", utf2bw(string_input[strings_y[0]][0]))\n",
    "        print(\"     \", utf2bw(ctable.decode(preds[0][0], calc_argmax=False)))\n",
    "\n",
    "\n",
    "#     print('Q', utf2bw(pretty_join(rowx)))\n",
    "\n",
    "    rowy = dict()\n",
    "    for i, v in enumerate(feat_y):\n",
    "        rowy[v] = {\"correct\": val[v].loc[ind]}\n",
    "        res = np.zeros((SENTLEN, rowy[v][\"correct\"].shape[1]))\n",
    "        for ii, c in enumerate(preds[i + 1][0]):\n",
    "            res[ii, c] = 1\n",
    "        rowy[v][\"pred\"] = pd.DataFrame(res, columns=val[v].columns)\n",
    "        results = []\n",
    "        if (rowy[v][\"correct\"].values == rowy[v][\"pred\"].values).all():\n",
    "            if printCorrect: print(colors.ok + '✅' + colors.close + v)\n",
    "        else:\n",
    "            isAllCorrect = False\n",
    "            print(colors.fail + '❌' + colors.close + v, end=' ')\n",
    "            #             results.append(colors.fail + '☒' + colors.close)\n",
    "            results.append('T ' + pretty_join(rowy[v][\"correct\"]))\n",
    "            results.append(pretty_join(rowy[v][\"pred\"]))\n",
    "            print(' '.join(results))\n",
    "    if isAllCorrect or times < 10:\n",
    "        print(\"\")\n",
    "        inspectOne(times + 1, printCorrect)\n",
    "\n",
    "inspectOne(printCorrect=False)\n",
    "\n",
    "with open(\n",
    "        'output' + datetime.datetime.now().strftime(\".%Y.%m.%d.%H.%M.%S\") +\n",
    "        '.txt', 'w') as f:\n",
    "    f.write(cap.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 10.2 Inspect All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "mydata = data[\"test\"]\n",
    "preds = model.predict(mydata[0])\n",
    "preds = [np.argmax(x, axis=-1) for x in preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def pretty_join2(arr, columns, v):\n",
    "    return \"/\".join([columns[row].replace(v+\"_\",\"\") for row in arr]).rstrip(\"/0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# for ite in range(len(mydata[0][strings_x[0]])):\n",
    "for ite in range(100):\n",
    "    print(\"\\nPredicted String Q\", ctable.decode(mydata[0][strings_x[0]][ite], calc_argmax=True), \"from\",\n",
    "          \"-\".join(str(x) for x in values_test.index.values[ite]))\n",
    "    \n",
    "    for i, v in enumerate(model.output_names):\n",
    "        if v not in strings_y:\n",
    "            continue\n",
    "\n",
    "        if not (np.argmax(mydata[1][v][ite], axis=-1) == preds[i][ite]).all():\n",
    "            print(colors.fail + '❌' + colors.close + v)\n",
    "            isAllCorrect = False\n",
    "            print(\"    T\", utf2bw(ctable.decode(mydata[1][v][ite], calc_argmax=True)))\n",
    "            print(\"     \", utf2bw(ctable.decode(preds[i][ite], calc_argmax=False)))\n",
    "\n",
    "    rowy = dict()\n",
    "    for i, v in enumerate(model.output_names):\n",
    "        if v in strings_y:\n",
    "            continue\n",
    "        rowy[v] = {\"correct\": np.argmax(mydata[1][v][ite], axis=-1)}\n",
    "        rowy[v][\"pred\"] = preds[i][ite]\n",
    "        results = []\n",
    "#         print(val[v].columns)\n",
    "        if not (rowy[v][\"correct\"] == rowy[v][\"pred\"]).all():\n",
    "            isAllCorrect = False\n",
    "            print(colors.fail + '❌' + colors.close + v, end=' ')\n",
    "            #             results.append(colors.fail + '☒' + colors.close)\n",
    "            results.append('T ' + pretty_join2(rowy[v][\"correct\"],val[v].columns,v))\n",
    "            results.append(pretty_join2(rowy[v][\"pred\"],val[v].columns,v))\n",
    "            print(' '.join(results))\n",
    "#         elif v ==\"QApos\":\n",
    "        else:\n",
    "            results.append(colors.ok + '✅' + colors.close + 'T ' + pretty_join2(rowy[v][\"correct\"],val[v].columns,v))\n",
    "            results.append(pretty_join2(rowy[v][\"pred\"],val[v].columns,v))\n",
    "            print(v, ' '.join(results))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "rowy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for _ in range(2):\n",
    "    inspectOne(print)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 11. Save Model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model.save(\"stringsAndCats.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(character_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ctable.decode(ctable.encode(\"ب\"), calc_argmax=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n",
    "### After alignment. Accuracy is good. Can be treated as baseline. (name=baseline)\n",
    "`strings_cats_aligned_2018.06.25.15.14`\n",
    "### Comaprison between baseline and POS embeddings. (No drop, so it is the best) (name=baseline+pos_emb)\n",
    "`with_pos_embeddings_2018.06.25.17.37`\n",
    "### Comaprison between baseline with POS embeddings and subword embeddings. (name=baseline+pos_emb+subword_emb)\n",
    "`with_embeddings_2018.06.25.17.37` NOT DONE\n",
    "### Comaprison between baseline with subword embeddings. (name=baseline+pos_emb+subword_emb+word2vec)\n",
    "`with_word2vec_2018.06.25.17.37` NOT DONE\n",
    "\n",
    "### Different Sizes of Baseline or subword"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "notify_time": "10"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

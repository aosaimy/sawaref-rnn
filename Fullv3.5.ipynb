{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Change Log (v.3.5)\n",
    "- LSTM for each sequence\n",
    "- Decoder for each output class (no gain)\n",
    "- Different weight loss (more for utf and pos) (no gain)\n",
    "\tacc\tacc\tacc\tacc\tacc\tacc\tacc\tacc\tacc\tacc\tall\tmf\tutf8\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "comp_end_decoder_for_each_test\t0.979865772\t0.957494407\t0.936017897\t0.968680089\t0.923489933\t0.942281879\t0.901118568\t0.936465324\t0.955704698\t0.670693512\t0.547651007\t0\t0.741834452\n",
    "comp_end_decoder_for_each_val\t0.978085868\t0.955724508\t0.932915921\t0.965116279\t0.920840787\t0.939624329\t0.900268336\t0.943202147\t0.957513417\t0.685152057\t0.569767442\t0\t0.744186047"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1. Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "format": "column",
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "# -*- coding: utf-8 -*-\n",
    "'''\n",
    "    An implementation of sequence to sequence learning\n",
    "    for performing ensemble morphosyntactic analyses\n",
    "'''\n",
    "from __future__ import print_function\n",
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "from six.moves import range\n",
    "from prepare_data import SawarefData, padIndexes\n",
    "from character_table import colors, CharacterTable, eprint\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import re\n",
    "from ws_client import WebSocketClient\n",
    "import pickle\n",
    "import sys\n",
    "import datetime\n",
    "from keras.callbacks import TensorBoard\n",
    "from buckwalter import utf2bw, bw2utf\n",
    "from pprint import pprint\n",
    "\n",
    "# do not import in interactive mode\n",
    "# from vis import SawarefVis\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras import layers\n",
    "from keras.callbacks import EarlyStopping, Callback\n",
    "from keras.utils import plot_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 2. Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    writer_all\n",
    "except NameError:\n",
    "    writer_all = pd.ExcelWriter('all_results.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Experiment:\n",
    "    feat_x = [\n",
    "        \"MXpos\", \"STpos\", \"AMpos\", \"FApos\", \n",
    "        \"STaspect\", \"AMaspect\", \"MXaspect\", \"FAaspect\", \n",
    "        \"STperson\", \"AMperson\", \"MXperson\", \"FAperson\", \n",
    "        \"STgender\", \"AMgender\", \"MXgender\", \"FAgender\", \n",
    "        \"STnumber\", \"AMnumber\", \"MXnumber\", \"FAnumber\", \n",
    "        \"STcase\", \"AMcase\", \"MXcase\", \"FAcase\", \n",
    "        \"STvoice\", \"AMvoice\", \"MXvoice\", \"FAvoice\", \n",
    "        \"STmood\", \"AMmood\", \"MXmood\", \"FAmood\", \n",
    "        \"STstate\", \"AMstate\", \"MXstate\", \"FAstate\"\n",
    "    ]\n",
    "    feat_y = [\n",
    "        \"QApos\", \"QAaspect\", \"QAperson\", \"QAgender\", \"QAnumber\", \"QAcase\",\n",
    "        \"QAvoice\", \"QAmood\", \"QAstate\"\n",
    "    ]\n",
    "    strings_x = [\"QAwutf8\"]\n",
    "    strings_y = [\"QAutf8\"]\n",
    "    MYPATH = \"/morpho/output/\"\n",
    "    # Parameters for the model and dataset.\n",
    "    TRAINING_SIZE = 50000\n",
    "    EPOCHS = 100\n",
    "    EMBEDDINGS = 0\n",
    "    # DIGITS = 3\n",
    "    # REVERSE = True\n",
    "    # Try replacing GRU, or SimpleRNN.\n",
    "    HIDDEN_SIZE = 128\n",
    "    BATCH_SIZE = 64\n",
    "    LAYERS = 1\n",
    "    ITERATIONS = 10\n",
    "    REVERSE = False\n",
    "    MODEL_NAME = \"main-seq-multiinput-multioutput-segmentation.keras\"\n",
    "    DATA_PICKLE = \"main-seq-multiinput-multioutput-segmentation.pickle\"\n",
    "    RNN = layers.LSTM\n",
    "    CATS_EMBEDDING = 1000\n",
    "    TEST_SPLIT = 1  #  means 0.1 of data is for test\n",
    "    VAL_SPLIT = 1  #  means 0.1 of data is for test\n",
    "    LOAD_FASTTEXT = False\n",
    "    CHARACTER_BASED = False\n",
    "    MORPHEME_BASED = False\n",
    "    previous_names = set()\n",
    "    \n",
    "    def __init__(self, exp_type=\"e\"):\n",
    "        self.exp_type = exp_type\n",
    "        self.set_constants()\n",
    "        self.set_name()\n",
    "        self.load_emb()\n",
    "        self.writer = pd.ExcelWriter('results_'+self.NAME+'.xlsx')\n",
    "    \n",
    "    def load_emb(self):\n",
    "        if self.LOAD_FASTTEXT:\n",
    "            self.emb = WebSocketClient(\"ws://localhost:8765/\")\n",
    "    def set_name(self):\n",
    "        self.NAME = self.exp_type+\"_\"+input(\"What is the name of this experiment? \")\n",
    "        self.thedate = datetime.datetime.now().strftime(\".%Y.%m.%d.%H.%M\")\n",
    "        print(\"\\nNAME=\", self.NAME)\n",
    "        print(\"DATE=\", self.thedate)\n",
    "        self.previous_names.add(self.NAME+\"%\"+self.thedate)\n",
    "    \n",
    "    def set_constants(self):\n",
    "        self.MYPATH = \"/morpho/output/\"\n",
    "        # Parameters for the model and dataset.\n",
    "        self.TRAINING_SIZE = 50000\n",
    "        self.EPOCHS = 40\n",
    "        self.EMBEDDINGS = 0\n",
    "        # DIGITS = 3\n",
    "        # REVERSE = True\n",
    "        # Try replacing GRU, or SimpleRNN.\n",
    "        self.HIDDEN_SIZE = 128\n",
    "        self.BATCH_SIZE = 64\n",
    "        self.LAYERS = 1\n",
    "        self.ITERATIONS = 10\n",
    "        self.MODEL_NAME = \"main-seq-multiinput-multioutput-segmentation.keras\"\n",
    "        self.DATA_PICKLE = \"main-seq-multiinput-multioutput-segmentation.pickle\"\n",
    "        self.RNN = layers.LSTM\n",
    "        if self.exp_type == \"comp_ru\" or self.exp_type == \"comp_sp1\":\n",
    "            self.CATS_EMBEDDING = 1000\n",
    "            self.TEST_SPLIT = 1  #  means 0.1 of data is for test\n",
    "            self.VAL_SPLIT = 1  #  means 0.1 of data is for test\n",
    "            self.LOAD_FASTTEXT = True\n",
    "            self.CHARACTER_BASED = False\n",
    "            self.MORPHEME_BASED = True\n",
    "            self.ALIGN_TYPE = \"ru-\" if self.exp_type == \"comp_ru\" else \"sp1-\"\n",
    "        elif self.exp_type == \"comp_ch\":\n",
    "            self.CATS_EMBEDDING = 1000\n",
    "            self.TEST_SPLIT = 1  #  means 0.1 of data is for test\n",
    "            self.VAL_SPLIT = 1  #  means 0.1 of data is for test\n",
    "            self.LOAD_FASTTEXT = False\n",
    "            self.CHARACTER_BASED = True\n",
    "            self.MORPHEME_BASED = False\n",
    "            self.ALIGN_TYPE = \"ch-\"\n",
    "        elif self.exp_type.startswith(\"comp_end\"):\n",
    "            self.CATS_EMBEDDING = 1000\n",
    "            self.TEST_SPLIT = 1  #  means 0.1 of data is for test\n",
    "            self.VAL_SPLIT = 1  #  means 0.1 of data is for test\n",
    "            self.LOAD_FASTTEXT = True\n",
    "            self.CHARACTER_BASED = False\n",
    "            self.MORPHEME_BASED = False\n",
    "            self.ALIGN_TYPE = \"\"\n",
    "\n",
    "            stripped = self.exp_type.lstrip(\"comp_end_\")\n",
    "            print(stripped)\n",
    "            if stripped in [\"MX\", \"ST\", \"FA\", \"AM\"]:\n",
    "                self.feat_x = [x for x in self.feat_x if x[0:2] == stripped]\n",
    "            elif len(stripped.split(\"_\")) >= 2:\n",
    "                self.feat_x = [x for x in self.feat_x if x[0:2] in stripped.split(\"_\")]\n",
    "            elif stripped == \"baseline\":\n",
    "                self.feat_x = []\n",
    "        else:\n",
    "            print(\"ERROR: Not a valid config name\")\n",
    "            raise NameError(\"ERROR: Not a valid config name\")\n",
    "        \n",
    "        if self.CHARACTER_BASED:\n",
    "            self.strings_x = [\"bw\"]\n",
    "        elif self.MORPHEME_BASED:\n",
    "            self.strings_x = [\"word\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "e = Experiment(\"comp_end\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "e.LOAD_MODEL = True if input(\"Load Model? (y/N)\") == \"y\" else False\n",
    "print(\"Loading Model\") if e.LOAD_MODEL else print(\"Training a new model. Old one with same name will be overwritten.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-23T15:55:05.579783Z",
     "start_time": "2018-07-23T15:55:05.576067Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Experiemnts Names\n",
    "### Compartative: \n",
    "- Morpheme-based (ru, sp1,),  `comp_ru`, `comp_sp1`, \n",
    "- character-based, `comp_ch`\n",
    "- end-to-end `comp_end`, \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 3. Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "`e.feat_x` is the input categorical features\n",
    "\n",
    "`e.feat_y` is the output categorical features\n",
    "\n",
    "`e.strings_x` is the input character-based strings features\n",
    "\n",
    "`e.strings_y` is the output character-based strings features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    accuracies\n",
    "except NameError:\n",
    "    accuracies = pd.DataFrame([],\n",
    "                 columns=list(map(list, zip(*[\n",
    "        ('QAaspect', 'acc'),\n",
    "        ('QAcase', 'acc'),\n",
    "        ('QAgender', 'acc'),\n",
    "        ('QAmood', 'acc'),\n",
    "        ('QAnumber', 'acc'),\n",
    "        ('QAperson', 'acc'),\n",
    "        ('QApos', 'acc'),\n",
    "        ('QAstate', 'acc'),\n",
    "        ('QAvoice', 'acc'),\n",
    "        ('QAutf8', 'acc'),\n",
    "        ('agg', 'all'),\n",
    "        ('agg', 'mf'),\n",
    "        ('agg', 'utf8')]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def pretty_join(arr):\n",
    "    if isinstance(arr, pd.Series):\n",
    "        arr = arr.to_frame().T\n",
    "    if isinstance(arr.columns, pd.core.index.MultiIndex):\n",
    "        return \"/\".join([\n",
    "            '+'.join([\n",
    "                x[1] for x in arr.columns[row == 1]\n",
    "                if x[1][-2:] != \"na\" and x[1][-2:] != \"_0\"\n",
    "            ]) for index, row in arr.iterrows()\n",
    "        ])\n",
    "    else:\n",
    "        return \"/\".join([\n",
    "            '+'.join([\n",
    "                x for x in arr.columns[row == 1]\n",
    "                if x[-2:] != \"na\" and x[-2:] != \"_0\"\n",
    "            ]) for index, row in arr.iterrows()\n",
    "        ])\n",
    "\n",
    "\n",
    "def pretty_value(colum_value):\n",
    "    return re.sub(\".*_\", \"\", colum_value)\n",
    "\n",
    "\n",
    "def getValuesAndReshape(df, middle_dim):\n",
    "    return df.values.reshape((df.shape[0] // middle_dim, middle_dim, -1))\n",
    "\n",
    "\n",
    "#     return df.values.reshape((df.shape[0]//middle_dim, middle_dim, df.shape[1]))\n",
    "\n",
    "\n",
    "def flattencolumns(df1, cols):\n",
    "    df = pd.concat(\n",
    "        [pd.DataFrame(df1[x].values.tolist()).add_prefix(x) for x in cols],\n",
    "        axis=1)\n",
    "    return pd.concat([df, df1.drop(cols, axis=1)], axis=1)\n",
    "\n",
    "\n",
    "def truncate(x):\n",
    "    return x[:EMBEDDINGS]\n",
    "\n",
    "\n",
    "def removeDiac(x):\n",
    "    return re.sub('[ًٌٍَُِّْ��]', '', x.replace(\"ٱ\",\"ا\"))\n",
    "\n",
    "\n",
    "def padStringWithSpaces(x):\n",
    "    return x + ' ' * (e.STRING_LENGTH - len(x))\n",
    "\n",
    "\n",
    "def joinMorphemesStrings(arr):\n",
    "    return \"+\".join([\n",
    "        x for x in arr if isinstance(x, float) == False and x != -1\n",
    "        and x != \"-----\" and x != \"-\"\n",
    "    ])\n",
    "\n",
    "\n",
    "def fullprint(*args, **kwargs):\n",
    "    opt = np.get_printoptions()\n",
    "    np.set_printoptions(threshold='nan')\n",
    "    pprint(*args, **kwargs)\n",
    "    np.set_printoptions(**opt)\n",
    "    \n",
    "    \n",
    "def pretty_join2(arr, columns, v):\n",
    "    return \"/\".join([columns[row].replace(v+\"_\",\"\") for row in arr]).rstrip(\"/0\")    \n",
    "\n",
    "\n",
    "class TestCallback(Callback):\n",
    "    def __init__(self, test_data):\n",
    "        self.test_data = test_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        x, y = self.test_data\n",
    "        metrics = self.model.evaluate(x, y, verbose=0)\n",
    "        for i,x in enumerate(self.model.metrics_names):\n",
    "            logs[\"test_\"+x]= metrics[i]\n",
    "        logs['test_acc']= np.mean([logs[\"test_\"+x] for x in self.model.metrics_names if x[-4:]=='_acc'])\n",
    "        logs['val_acc']= np.mean([logs[\"val_\"+x] for x in self.model.metrics_names if x[-4:]=='_acc'])\n",
    "        logs['train_acc']= np.mean([logs[x] for x in self.model.metrics_names if x[-4:]=='_acc'])\n",
    "        return logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 4. Loading data from sawaref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    results_all\n",
    "except NameError:\n",
    "    results_all = {}\n",
    "if e.NAME in results_all:\n",
    "    print(\"Warning: same name exits. Results will be overwritten\")\n",
    "else:\n",
    "    results_all[e.NAME] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sawarefData = SawarefData(\n",
    "    e.MYPATH,\n",
    "    e.EMBEDDINGS,\n",
    "    align_type='no-' if e.ALIGN_TYPE == '' else e.ALIGN_TYPE,\n",
    "    feat_x=e.feat_x,\n",
    "    strings_x=e.strings_x,\n",
    "    strings_y=e.strings_y,\n",
    "    feat_y=e.feat_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "source = list(itertools.chain(*sawarefData.quran_sent))\n",
    "df = pd.DataFrame(\n",
    "    source,\n",
    "    columns=[\"sid\", \"aid\", \"wid\", \"mid\"] + e.feat_x + e.strings_x + e.strings_y +\n",
    "    [\"embeddings\"] + e.feat_y)\n",
    "if e.EMBEDDINGS > 0:\n",
    "    df[\"embeddings\"] = df[\"embeddings\"].apply(truncate)\n",
    "    df = flattencolumns(df, [\"embeddings\"])\n",
    "df.set_index([\"sid\", \"aid\", \"wid\", \"mid\"], inplace=True)\n",
    "df.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# should be df.replace but it is not working due to a bug\n",
    "for x in df.columns:\n",
    "    df.loc[df[x].astype(str) == \"0\", x] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "strings = e.strings_x + e.strings_y\n",
    "\n",
    "# a. clean all padded rows\n",
    "for s in strings:\n",
    "    #df.loc[df2[(\"vals\", s)] == -1, (\"vals\", s)] = -1\n",
    "    df.loc[pd.isna(df[s]), s] = \"\"\n",
    "\n",
    "for x in strings:\n",
    "    df[x+\"_undiac\"] = df[x].apply(removeDiac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## 2. Pad the rows according to the longest word (in # of morphemes)\n",
    "e.SENTLEN = max(df.index.get_level_values(\"mid\"))\n",
    "df = df.reindex(\n",
    "    padIndexes(df, max(df.index.get_level_values(\"mid\"))),\n",
    "    fill_value=-1).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## 3. Get the hot encoding of all caterogirical data (see columns attr)\n",
    "dumm = pd.get_dummies(df, columns=e.feat_x + e.feat_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for x in e.feat_x:\n",
    "    dumm.drop(x+\"_-1\",axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dumm.drop(\"embeddings\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## 4. Add two-level columns for easy indexing later (wid, mid)\n",
    "e.EXAMPLES_LEN = df.shape[0] // e.SENTLEN\n",
    "new_columns = []\n",
    "for x in dumm.columns:\n",
    "    new_columns.append(re.sub('(_.*|[0-9]*)', '', x))\n",
    "dumm.columns = [new_columns, dumm.columns]\n",
    "dumm.index = [[x for x in range(e.EXAMPLES_LEN) for _ in range(e.SENTLEN)],\n",
    "              [x for _ in range(e.EXAMPLES_LEN) for x in range(e.SENTLEN)]]\n",
    "dumm.sort_index(axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## 5. Find possible values of each cat\n",
    "def getSet(df):\n",
    "    results = set()\n",
    "    df.apply(results.add)\n",
    "    return results\n",
    "\n",
    "\n",
    "embeddingInputSets = {i: getSet(df[i]) for i in e.feat_x}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "e.feat_x = list(set(e.feat_x) - set([x for x in e.feat_x if len( embeddingInputSets[x])<=1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df2 = pd.concat([df.reset_index(), dumm.reset_index()], axis=1)\n",
    "# df2.set_index([\"sid\", \"aid\", \"wid\", \"mid\"], inplace=True)\n",
    "df2.index = [[x for x in range(e.EXAMPLES_LEN) for _ in range(e.SENTLEN)],\n",
    "             [x for _ in range(e.EXAMPLES_LEN) for x in range(e.SENTLEN)]]\n",
    "\n",
    "if len(['embeddings' + str(x) for x in range(e.EMBEDDINGS)]) > 0:\n",
    "    df2.drop(['embeddings' + str(x) for x in range(e.EMBEDDINGS)], inplace=True, axis=1)\n",
    "# df2.columns = [(x,\"val\") if isinstance(x,str) else x  for x in df2.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df2.columns = [[\"vals\" if isinstance(x, str) else x[0] for x in df2.columns],\n",
    "               [x if isinstance(x, str) else x[1] for x in df2.columns]]\n",
    "\n",
    "df2.sort_index(axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "del df\n",
    "del dumm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def emb_encode(x):\n",
    "    return np.zeros(e.emb.vector_size) if re.sub(\"\\\\s*\",\"\",x)==\"\" or x == -1 else e.emb[x]\n",
    "\n",
    "def convertStringToFeatX(x):\n",
    "    df2[(x,\"emb\")] = df2[(x,x+\"_undiac\")].apply(emb_encode)\n",
    "    return df2.join(pd.DataFrame(np.stack(df2[(x,\"emb\")].values),\n",
    "                         index=df2.index, \n",
    "                         columns=[[x+\"_emb\" for i in range(emb.vector_size)],\n",
    "                                  [x+\"_emb_\"+str(i) for i in range(emb.vector_size)]]))\n",
    "\n",
    "# if not e.CHARACTER_BASED and e.MORPHEME_BASED:\n",
    "#     df2 = convertStringToFeatX(\"word\")\n",
    "#     e.feat_x.append(\"word_emb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# b. group them by morpheme and join with \"+\"\"\n",
    "df_strings = pd.DataFrame({\n",
    "    x: df2[\"vals\", x].groupby(level=[0]).apply(joinMorphemesStrings)\n",
    "    for x in strings\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# c. pad joined morphemes\n",
    "e.STRING_LENGTH = max([len(x) for k in strings for x in df_strings[k]])\n",
    "for s in strings:\n",
    "    df_strings[s] = df_strings[s].apply(padStringWithSpaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# d. encode them in one hot encoding\n",
    "charset = set(\"+\").union(\n",
    "    *[list(set(\"\".join(df_strings[x] + \"-\"))) for x in strings])\n",
    "ctable = CharacterTable(charset, e.STRING_LENGTH)\n",
    "### Now we have one shape for all strings: (STRING_LENGTH, len(charset))\n",
    "for x in strings:\n",
    "    df_strings[x + \"_onehot\"] = df_strings[x].apply(ctable.encode)\n",
    "df_strings['num'] = [x for x in range(len(df_strings))]\n",
    "df_strings.set_index('num', append=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# e. remove diac\n",
    "for x in strings:\n",
    "    df_strings[x+ \"_undiac\"] = df_strings[x].apply(removeDiac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# f. encode them as dense vector using fastText\n",
    "for x in e.strings_x:\n",
    "    if e.LOAD_FASTTEXT:\n",
    "        df_strings[x + \"_emb\"] = df_strings[x+\"_undiac\"].apply(emb_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def emb_encode_char(x):\n",
    "    return ctable.encode(x)[0].argmax()\n",
    "if e.CHARACTER_BASED:\n",
    "    df2[\"bw_onehot\"] = df2[(\"bw\",\"bw\")].astype(str).apply(emb_encode_char)\n",
    "    df2.filter(regex=\"bw\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 6. Prepare splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df2.filter(regex=\"(FAnumber|.*utf8|.id)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for x in sorted(e.feat_x): \n",
    "    if len(df2[x].columns)  == 1:\n",
    "        print(x, len(df2[x].columns))\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# 6. Shuffle (x, y) in unison\n",
    "np.random.seed(0)\n",
    "indices = list(range(e.EXAMPLES_LEN))\n",
    "np.random.shuffle(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# 7. Explicitly set apart 10% for validation data that we never train over.\n",
    "test_split_at = int(e.EXAMPLES_LEN * e.TEST_SPLIT / 10)\n",
    "val_split_at = int(e.EXAMPLES_LEN * e.TEST_SPLIT / 10 +\n",
    "                   e.EXAMPLES_LEN * e.VAL_SPLIT / 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def getMorphemeBasedIndeicies(arr):\n",
    "    return np.array([list(range(x*e.SENTLEN,x*e.SENTLEN+e.SENTLEN)) for x in arr]).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "values_test = df_strings.iloc[indices[:test_split_at]]\n",
    "values_val = df_strings.iloc[indices[test_split_at:val_split_at]]\n",
    "values_train = df_strings.iloc[indices[val_split_at:]]\n",
    "\n",
    "test = df2.iloc[getMorphemeBasedIndeicies(indices[:test_split_at])]\n",
    "val = df2.iloc[getMorphemeBasedIndeicies(indices[test_split_at:val_split_at])]\n",
    "train = df2.iloc[getMorphemeBasedIndeicies(indices[val_split_at:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "embeddingInputLists = {\n",
    "    i: [i + \"_\" + str(x) for x in embeddingInputSets[i]]\n",
    "    for i in embeddingInputSets\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def getValuesAndReshape(df, middle_dim, test=\"\"):\n",
    "    return df.values.reshape((df.shape[0] // middle_dim, middle_dim, -1))\n",
    "\n",
    "def setZero(df,zeros, from_val=-1, to_val=0.):\n",
    "#     dumm[(dumm.isin([0,-1])).all(axis=1)] = -1.\n",
    "    if zeros:\n",
    "        df[(df==from_val).all(axis=1)] = to_val\n",
    "    return df\n",
    "\n",
    "def getData(cat_source, str_source, cats_feats=[], strs_feats=[], embeddings=False, zeros= False):\n",
    "    data = {\n",
    "        **{i:\n",
    "           getValuesAndReshape(cat_source[i].idxmax(axis=1).apply(lambda x: embeddingInputLists[i].index(x)),e.SENTLEN)\n",
    "               if i in embeddingInputSets and len(embeddingInputSets[i]) > 10000+e.CATS_EMBEDDING #and i.replace(\"_emb\",\"\") not in e.strings_x\n",
    "               else getValuesAndReshape(cat_source[i], e.SENTLEN,i) \n",
    "           for i in cats_feats},\n",
    "        **{i:np.stack(str_source[i+\"_onehot\"].values) for i in strs_feats},\n",
    "    }\n",
    "    if embeddings:\n",
    "        for i in strs_feats:\n",
    "            if i+\"_emb\" not in data:\n",
    "                data[i+\"_emb\"] = np.stack(str_source[i+\"_emb\"].values)\n",
    "    if e.CHARACTER_BASED:\n",
    "        data[\"bw_onehot\"] = cat_source[\"bw_onehot\"].values.reshape((-1, e.SENTLEN))\n",
    "    if zeros:\n",
    "        for i in cats_feats:\n",
    "            data[i][np.where(data[i]==-1.)] = 0.\n",
    "    return data\n",
    "\n",
    "\n",
    "data = {\n",
    "    'input': getData(train, values_train, cats_feats=e.feat_x, strs_feats=e.strings_x, embeddings=e.LOAD_FASTTEXT),\n",
    "    'output': getData(train, values_train, cats_feats=e.feat_y, strs_feats=e.strings_y, zeros=True),\n",
    "    'val': (\n",
    "        getData(val, values_val, cats_feats=e.feat_x, strs_feats=e.strings_x, embeddings=e.LOAD_FASTTEXT),\n",
    "        getData(val, values_val, cats_feats=e.feat_y, strs_feats=e.strings_y, zeros=True)\n",
    "    ),\n",
    "    'test': (\n",
    "        getData(test, values_test, cats_feats=e.feat_x, strs_feats=e.strings_x, embeddings=e.LOAD_FASTTEXT),\n",
    "        getData(test, values_test, cats_feats=e.feat_y, strs_feats=e.strings_y, zeros=True)\n",
    "    )\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data[\"val\"] = tuple([data[\"val\"][0],data[\"val\"][1],\n",
    "                     {i: (data[\"val\"][1][i].argmax(axis=2) > 0).astype(int) for i in e.feat_y + e.strings_y if i in data[\"val\"][1]},]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 8. Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 8.1 End-To-End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def getEmbedding(input):\n",
    "    name = input.name.split(\"_\")[0]\n",
    "    if name in e.feat_x and len(embeddingInputSets[name]) > e.CATS_EMBEDDING:\n",
    "        return layers.Reshape((e.SENTLEN, -1))(layers.Embedding(len(embeddingInputSets[name]),2, input_length=e.SENTLEN)(input))\n",
    "    else:\n",
    "        return layers.Dropout(0.1)(input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if not e.LOAD_MODEL and not e.MORPHEME_BASED and not e.CHARACTER_BASED:\n",
    "    # print('Build model...')\n",
    "    outputs = []\n",
    "    inputs = []\n",
    "    # For strings\n",
    "    strings_input = layers.Input(shape=(e.STRING_LENGTH, len(charset)), name=e.strings_x[0])\n",
    "    lstm_strings_encoder = layers.Bidirectional(e.RNN(e.HIDDEN_SIZE,name=\"lstm_strings_encoder\"))(strings_input)\n",
    "\n",
    "    if e.LOAD_FASTTEXT:\n",
    "        emb_input = layers.Input(shape=(e.emb.vector_size,), name=e.strings_x[0]+\"_emb\")\n",
    "        inputs.append(emb_input)\n",
    "    if len(e.feat_x) == 0:\n",
    "        inputs.append(strings_input)\n",
    "        concatenated = layers.Concatenate()([lstm_strings_encoder,emb_input])\n",
    "    else:\n",
    "        # For categoricals\n",
    "        feat_x_cat = {name: [x for x in e.feat_x if x[:2] == name] for name in set([y[:2] for y in e.feat_x]) }\n",
    "        rnns = []\n",
    "        for i in feat_x_cat:\n",
    "            inp = []\n",
    "            for j in feat_x_cat[i]:\n",
    "                inp.append(layers.Input(shape=(e.SENTLEN, data[\"input\"][j].shape[2]), name=j))\n",
    "\n",
    "            # main_input = layers.Concatenate()([layers.Dropout(0.1)(input) for input in inputs])\n",
    "            input_list = [getEmbedding(input) for input in inp]\n",
    "\n",
    "            main_input = layers.Concatenate()(input_list)\n",
    "            lstm_out = layers.Bidirectional(e.RNN(e.HIDDEN_SIZE), name=\"RNN_\"+i)(main_input)\n",
    "            rnns.append(lstm_out)\n",
    "            inputs +=inp\n",
    "\n",
    "        inputs.append(strings_input)\n",
    "\n",
    "        # input_shape=(None, len(ctable_x.chars) + EMBEDDINGS)))\n",
    "        # As the decoder e.RNN's input, repeatedly provide with the last hidden state of\n",
    "        # e.RNN for each time step. Repeat 'DIGITS + 1' times as that's the maximum\n",
    "        # length of output, e.g., when DIGITS=3, max output is 999+999=1998.\n",
    "        if e.LOAD_FASTTEXT:\n",
    "            concatenated = layers.Concatenate()(rnns+[lstm_strings_encoder,emb_input])\n",
    "        else:\n",
    "            concatenated = layers.Concatenate()(rnns+[lstm_strings_encoder])\n",
    "\n",
    "    # For strings again\n",
    "    repeat_strings_out = layers.RepeatVector(e.STRING_LENGTH)(concatenated)\n",
    "    rnn_out = e.RNN(e.HIDDEN_SIZE, return_sequences=True)(repeat_strings_out)\n",
    "    strings_output = layers.TimeDistributed(\n",
    "          layers.Dense(\n",
    "            len(charset), \n",
    "            activation=\"softmax\"), name=e.strings_y[0])(rnn_out)\n",
    "    outputs.append(strings_output)\n",
    "\n",
    "    dropout_out = layers.Dropout(0.5)(concatenated)\n",
    "    repeat_out = layers.RepeatVector(e.SENTLEN)(dropout_out)\n",
    "    # The decoder e.RNN could be multiple layers stacked or a single layer.\n",
    "    rnn_out = e.RNN(e.HIDDEN_SIZE, return_sequences=True)(repeat_out)\n",
    "    for _ in range(e.LAYERS-1):\n",
    "        # By setting return_sequences to True, return not only the last output but\n",
    "        # all the outputs so far in the form of (num_samples, timesteps,\n",
    "        # output_dim). This is necessary as TimeDistributed in the below expects\n",
    "        # the first dimension to be the timesteps.\n",
    "        rnn_out = e.RNN(e.HIDDEN_SIZE, return_sequences=True)(rnn_out)\n",
    "\n",
    "\n",
    "\n",
    "    # Apply a dense layer to the every temporal slice of an input. For each of step\n",
    "    # of the output sequence, decide which character should be chosen.\n",
    "\n",
    "    for i in e.feat_y:\n",
    "        outputs.append(\n",
    "          layers.TimeDistributed(\n",
    "          layers.Dense(\n",
    "            data[\"output\"][i].shape[2], \n",
    "            activation=\"softmax\"), name=i)(rnn_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1a Decoder For Each Output Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "if not e.LOAD_MODEL and not e.MORPHEME_BASED and not e.CHARACTER_BASED:\n",
    "    # print('Build model...')\n",
    "    outputs = []\n",
    "    inputs = []\n",
    "    # For strings\n",
    "    strings_input = layers.Input(shape=(e.STRING_LENGTH, len(charset)), name=e.strings_x[0])\n",
    "    lstm_strings_encoder = layers.Bidirectional(e.RNN(e.HIDDEN_SIZE,name=\"lstm_strings_encoder\"))(strings_input)\n",
    "\n",
    "    if e.LOAD_FASTTEXT:\n",
    "        emb_input = layers.Input(shape=(e.emb.vector_size,), name=e.strings_x[0]+\"_emb\")\n",
    "        inputs.append(emb_input)\n",
    "    if len(e.feat_x) == 0:\n",
    "        inputs.append(strings_input)\n",
    "        concatenated = layers.Concatenate()([lstm_strings_encoder,emb_input])\n",
    "    else:\n",
    "        # For categoricals\n",
    "        feat_x_cat = {name: [x for x in e.feat_x if x[:2] == name] for name in set([y[:2] for y in e.feat_x]) }\n",
    "        rnns = []\n",
    "        for i in feat_x_cat:\n",
    "            inp = []\n",
    "            for j in feat_x_cat[i]:\n",
    "                inp.append(layers.Input(shape=(e.SENTLEN, data[\"input\"][j].shape[2]), name=j))\n",
    "\n",
    "            # main_input = layers.Concatenate()([layers.Dropout(0.1)(input) for input in inputs])\n",
    "            input_list = [getEmbedding(input) for input in inp]\n",
    "\n",
    "            main_input = layers.Concatenate()(input_list)\n",
    "            lstm_out = layers.Bidirectional(e.RNN(e.HIDDEN_SIZE), name=\"RNN_\"+i)(main_input)\n",
    "            rnns.append(lstm_out)\n",
    "            inputs +=inp\n",
    "\n",
    "        inputs.append(strings_input)\n",
    "\n",
    "        # input_shape=(None, len(ctable_x.chars) + EMBEDDINGS)))\n",
    "        # As the decoder e.RNN's input, repeatedly provide with the last hidden state of\n",
    "        # e.RNN for each time step. Repeat 'DIGITS + 1' times as that's the maximum\n",
    "        # length of output, e.g., when DIGITS=3, max output is 999+999=1998.\n",
    "        if e.LOAD_FASTTEXT:\n",
    "            concatenated = layers.Concatenate()(rnns+[lstm_strings_encoder,emb_input])\n",
    "        else:\n",
    "            concatenated = layers.Concatenate()(rnns+[lstm_strings_encoder])\n",
    "\n",
    "    # For strings again\n",
    "    repeat_strings_out = layers.RepeatVector(e.STRING_LENGTH)(concatenated)\n",
    "    rnn_out = e.RNN(e.HIDDEN_SIZE, return_sequences=True)(repeat_strings_out)\n",
    "    strings_output = layers.TimeDistributed(\n",
    "          layers.Dense(\n",
    "            len(charset), \n",
    "            activation=\"softmax\"), name=e.strings_y[0])(rnn_out)\n",
    "    outputs.append(strings_output)\n",
    "\n",
    "    dropout_out = layers.Dropout(0.5)(concatenated)\n",
    "    repeat_out = layers.RepeatVector(e.SENTLEN)(dropout_out)\n",
    "\n",
    "    # Apply a dense layer to the every temporal slice of an input. For each of step\n",
    "    # of the output sequence, decide which character should be chosen.\n",
    "\n",
    "    for i in e.feat_y:\n",
    "        # The decoder e.RNN could be multiple layers stacked or a single layer.\n",
    "        rnn_out = e.RNN(e.HIDDEN_SIZE, return_sequences=True)(repeat_out)\n",
    "        for _ in range(e.LAYERS-1):\n",
    "            # By setting return_sequences to True, return not only the last output but\n",
    "            # all the outputs so far in the form of (num_samples, timesteps,\n",
    "            # output_dim). This is necessary as TimeDistributed in the below expects\n",
    "            # the first dimension to be the timesteps.\n",
    "            rnn_out = e.RNN(e.HIDDEN_SIZE, return_sequences=True)(rnn_out)\n",
    "        outputs.append(\n",
    "          layers.TimeDistributed(\n",
    "          layers.Dense(\n",
    "            data[\"output\"][i].shape[2], \n",
    "            activation=\"softmax\"), name=i)(rnn_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-22T12:53:33.291031Z",
     "start_time": "2018-07-22T12:53:33.287984Z"
    },
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 8.2 Morpheme-Based  or 8.3 Character-Based "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if not e.LOAD_MODEL and e.MORPHEME_BASED or e.CHARACTER_BASED:\n",
    "    # print('Build model...')\n",
    "    outputs = []\n",
    "    inputs = []\n",
    "\n",
    "    # For categoricals\n",
    "    for i in e.feat_x:\n",
    "        inputs.append(layers.Input(shape=(e.SENTLEN, data[\"input\"][i].shape[2]), name=i))\n",
    "\n",
    "    def getEmbeddingWithMasking(input):\n",
    "        name = input.name.split(\"_\")[0]\n",
    "        if name in e.feat_x and len(embeddingInputSets[name]) > e.CATS_EMBEDDING:\n",
    "            return layers.Masking(mask_value=0.)(layers.Reshape((e.SENTLEN, -1))(layers.Embedding(len(embeddingInputSets[name]),2, input_length=SENTLEN)(input)))\n",
    "        else:\n",
    "            return layers.Dropout(0.1)(layers.Masking(mask_value=0.)(input))\n",
    "\n",
    "    main_input = layers.Concatenate()([getEmbedding(input) for input in inputs])\n",
    "\n",
    "    lstm_out = layers.Bidirectional(e.RNN(e.HIDDEN_SIZE, return_sequences=True))(main_input)\n",
    "    if e.LOAD_FASTTEXT and e.MORPHEME_BASED:\n",
    "        emb_input = layers.Input(shape=(e.emb.vector_size,), name=e.strings_x[0]+\"_emb\")\n",
    "        inputs.append(emb_input)\n",
    "        rnn_out = layers.Concatenate()([lstm_out,layers.RepeatVector(e.SENTLEN)(emb_input)])\n",
    "    elif e.CHARACTER_BASED:\n",
    "        emb_input = layers.Input(shape=(e.SENTLEN,), name=\"bw_onehot\")\n",
    "        inputs.append(emb_input)\n",
    "        rnn_out = layers.Concatenate()([lstm_out,layers.Embedding(len(ctable.chars),10)(emb_input)])\n",
    "    else:\n",
    "        rnn_out = lstm_out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Apply a dense layer to the every temporal slice of an input. For each of step\n",
    "    # of the output sequence, decide which character should be chosen.\n",
    "\n",
    "    for i in e.feat_y:\n",
    "        outputs.append(\n",
    "          layers.TimeDistributed(\n",
    "          layers.Dense(\n",
    "            data[\"output\"][i].shape[2], \n",
    "            activation=\"softmax\"), name=i)(rnn_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if not e.LOAD_MODEL:\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    model.compile(sample_weight_mode=\"temporal\", #if not e.MORPHEME_BASED and not e.CHARACTER_BASED else None,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  loss_weights={x: 1.0 if x in [\"QAutf8\",\"QApos\"] else 0.2 for x in model.output_names},\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy']\n",
    "                )\n",
    "else:\n",
    "    from os.path import exists\n",
    "    from os import listdir\n",
    "    print(\"models/\" + e.NAME + \"_{}.keras\".format(e.thedate))\n",
    "    if exists(\"models/\" + e.NAME + \"_{}.keras\".format(e.thedate)):\n",
    "        model = load_model(\"models/\" + e.NAME + \"_{}.keras\".format(e.thedate))\n",
    "    else:\n",
    "        print(\"possible models: (please change the date to reload model)\")\n",
    "        print([f for f in listdir(\"models/\") if f.startswith(e.NAME)])\n",
    "        d = input(\"the new date? (empty to keep the original one)\")\n",
    "        if d != \"\":\n",
    "            e.thedate = \".\"+d\n",
    "            print(\"date has been changed. Please re-run this code\")\n",
    "        else:\n",
    "            print(\"date has not been changed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 9. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "format": "column"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "if not e.LOAD_MODEL:\n",
    "    plot_model(model, to_file='plots/model_{}.png'.format(e.NAME), )#show_shapes=True)\n",
    "SVG(model_to_dot(model, show_shapes=True, show_layer_names=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 10. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# optional\n",
    "e.set_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if not e.LOAD_MODEL:\n",
    "    earlyStopping=EarlyStopping(monitor='val_loss', patience=5, verbose=0, mode='auto')\n",
    "\n",
    "    tensorboard = TensorBoard(\n",
    "        log_dir=\"logs/\" + e.NAME + \"_{}\".format(e.thedate))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(e.NAME + \"_{}\".format(e.thedate))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not e.LOAD_MODEL:\n",
    "    history = model.fit(data['input'], data['output'],\n",
    "                    batch_size=e.BATCH_SIZE,\n",
    "                    shuffle=False,\n",
    "                    callbacks=[earlyStopping, TestCallback(data['test']), tensorboard],\n",
    "                    epochs=e.EPOCHS,\n",
    "                    verbose=2,\n",
    "                    sample_weight={i:((data[\"output\"][i].argmax(axis=2) > 0).astype(int)*.9)+.1 for i in model.output_names},\n",
    "                    validation_data=data['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if not e.LOAD_MODEL:\n",
    "    model.save(\"models/\" + e.NAME + \"_{}.keras\".format(e.thedate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def calc_accuracy(model,mydata,data_length, debug=True):\n",
    "    preds = [np.argmax(x, axis=-1) for x in model.predict(mydata[0])]\n",
    "    results = pd.DataFrame([], columns=[[\"inputs\"]*2+[\"agg\"]*3+model.output_names*3\n",
    "                                        ,[\"QAwutf8\",\"index\"]\n",
    "                                        +[\"all\",\"utf8\",\"mf\"]\n",
    "                                        +[\"acc\"]*len(model.output_names)\n",
    "                                        +[\"pred\"]*len(model.output_names)\n",
    "                                        +[\"actu\"]*len(model.output_names)])\n",
    "    results.sort_index(axis=1, inplace=True)\n",
    "    for ite in range(data_length):\n",
    "        df2_row = mydata[2].iloc[ite*5]\n",
    "        r = {\n",
    "            (\"inputs\",\"QAwutf8\"): ctable.decode(mydata[0][e.strings_x[0]][ite], calc_argmax=True),\n",
    "            (\"inputs\",\"index\"): str(df2_row[(\"vals\",\"sid\")])+\"-\"+str(df2_row[(\"vals\",\"aid\")])+\"-\"+str(df2_row[(\"vals\",\"wid\")])+\"-\"+str(df2_row[(\"vals\",\"mid\")])\n",
    "        }\n",
    "        for i, v in enumerate(model.output_names):\n",
    "            if v not in e.strings_y:\n",
    "                continue\n",
    "\n",
    "            r[(v,\"acc\")] = (np.argmax(mydata[1][v][ite], axis=-1) == preds[i][ite]).all()\n",
    "            r[(v,\"pred\")] = utf2bw(ctable.decode(preds[i][ite], calc_argmax=False)).strip(\" \") if debug else \"\"\n",
    "            r[(v,\"actu\")] = utf2bw(ctable.decode(mydata[1][v][ite], calc_argmax=True)).strip(\" \") if debug else \"\"\n",
    "\n",
    "        for i, v in enumerate(model.output_names):\n",
    "            if v in e.strings_y:\n",
    "                continue\n",
    "            correct, pred = np.argmax(mydata[1][v][ite], axis=-1), preds[i][ite]\n",
    "            r[(v,\"acc\")] = (correct == pred).all()\n",
    "            r[(v,\"actu\")] = pretty_join2(correct,val[v].columns,v) if debug else \"\"\n",
    "            r[(v,\"pred\")] = pretty_join2(pred,val[v].columns,v) if debug else \"\"\n",
    "        r[(\"agg\",\"all\")] = sum([r[(v,\"acc\")] for i, v in enumerate(model.output_names)]) == len(model.output_names)\n",
    "        r[(\"agg\",\"utf8\")] = sum([r[(v,\"acc\")] for i, v in enumerate(model.output_names) if v != \"QAutf8\" ]) == len([x for i,v in enumerate(model.output_names) if v != \"QAutf8\"])\n",
    "        r[(\"agg\",\"mf\")] = sum([r[(v,\"acc\")] for i, v in enumerate(model.output_names)  if v != \"QAutf8\" and v != \"QApos\" ]) == len([x for i,v in enumerate(model.output_names) if v != \"QApos\"])\n",
    "        results.loc[ite] = r\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "result = calc_accuracy(model, [*data[\"test\"],test], len(data[\"test\"][0][e.strings_x[0]]),debug=True)\n",
    "result.to_excel(e.writer,'results')\n",
    "e.writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    results\n",
    "except NameError:\n",
    "    results = {}\n",
    "\n",
    "if e.NAME not in results:\n",
    "    results[e.NAME] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "results[e.NAME][\"results_test\"]=result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "r = {x: np.average(result[x].values.astype(int),axis=0) if x in result.columns else 0 for x in list(accuracies.columns)}\n",
    "\n",
    "accuracies.loc[e.NAME+\"_test\"] = r\n",
    "accuracies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "result = calc_accuracy(model, [data[\"val\"][0],data[\"val\"][1],val], len(data[\"val\"][0][e.strings_x[0]]),debug=True)\n",
    "result.to_excel(e.writer,'results_val')\n",
    "e.writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "results[e.NAME][\"results_val\"]=result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "r = {x: np.average(result[x].values.astype(int),axis=0) if x in result.columns else 0 for x in list(accuracies.columns)}\n",
    "\n",
    "accuracies.loc[e.NAME+\"_val\"] = r\n",
    "accuracies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "accuracies.to_excel(writer_all,'acc')\n",
    "writer_all.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.plot(history.history['test_loss'])\n",
    "plt.title('model overall loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val', 'test'], loc='upper right')\n",
    "plt.savefig('plots/model_overall_loss' +\n",
    "            e.NAME + \"_{}\".format(e.thedate) + '.png')\n",
    "plt.close(fig)\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# In[471]:\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history[\"train_acc\"])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.plot(history.history['test_acc'])\n",
    "plt.title('model average accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend([x +\" = \"+ str(round(history.history[x+'_acc'][-1]*100,2)) for x in ['train', 'val', 'test']], loc='lower right')\n",
    "# for i, x in enumerate(['train', 'val', 'test']):\n",
    "#     plt.annotate(str(round(history.history[x+'_acc'][-1]*100,2)),\n",
    "#                  xy=(len(history.history[x+'_acc']), history.history[x+'_acc'][-1]), \n",
    "#                  textcoords='figure pixels', \n",
    "#                  xytext=(-20,-10))\n",
    "plt.savefig('plots/model_average_accuracy' +\n",
    "            e.NAME + \"_{}\".format(e.thedate) + '.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.plot(history.history['test_loss'])\n",
    "plt.title('model overall loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val', 'test'], loc='upper right')\n",
    "plt.savefig('plots/model_overall_loss' +\n",
    "            e.NAME + \"_{}\".format(e.thedate) + '.png')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history[\"train_acc\"])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.plot(history.history['test_acc'])\n",
    "plt.title('model average accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend([x +\" = \"+ str(round(history.history[x+'_acc'][-1]*100,2)) for x in ['train', 'val', 'test']], loc='lower right')\n",
    "# for i, x in enumerate(['train', 'val', 'test']):\n",
    "#     plt.annotate(str(round(history.history[x+'_acc'][-1]*100,2)),\n",
    "#                  xy=(len(history.history[x+'_acc']), history.history[x+'_acc'][-1]), \n",
    "#                  textcoords='figure pixels', \n",
    "#                  xytext=(-20,-10))\n",
    "plt.savefig('plots/model_average_accuracy' +\n",
    "            e.NAME + \"_{}\".format(e.thedate) + '.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# VALIDATAION = True\n",
    "# prefix = \"val_\" if VALIDATAION else \"\"\n",
    "# del prefix\n",
    "legends = []\n",
    "for x in model.output_names:\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(history.history[x+\"_acc\"])\n",
    "    legends.append(\"\"+x +\" = \"+ str(round(history.history[x+'_acc'][-1]*100,2)))\n",
    "#     plt.plot(history.history[x+\"_acc\"])\n",
    "#     legends.append(\"val_\"+x)\n",
    "#     plt.plot(history.history[\"val_\" + x+\"_acc\"])\n",
    "#     legends.append(\"train_\"+x)\n",
    "plt.title('model indiviual accuracy on test dataset')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(legends, loc='lower right')\n",
    "plt.savefig('plots/accuracy_all' +\n",
    "            e.NAME + \"_{}\".format(e.thedate) + '.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "legends = []\n",
    "for x in model.output_names:\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history[\"test_\"+x+\"_loss\"])\n",
    "    legends.append(\"\"+x +\" = \"+ str(round(history.history[x+'_loss'][-1]*100,2)))\n",
    "plt.title('model individual loss on test dataset')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(legends, loc='upper right')\n",
    "plt.savefig('plots/loss_all' +\n",
    "            e.NAME + \"_{}\".format(e.thedate) + '.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 10.1 Inspect One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#%%capture --no-stderr cap\n",
    "%autoreload 2\n",
    "\n",
    "colors.ok = ''\n",
    "colors.fail = ''\n",
    "colors.close = ''\n",
    "\n",
    "\n",
    "def inspectOne(times=0, printCorrect=True):\n",
    "    isAllCorrect = True\n",
    "    if times > 10000: # to prevent infinite loops\n",
    "        return\n",
    "    ind = np.random.randint(0, len(val.index))\n",
    "    ind = (val.index[ind][0], slice(None))\n",
    "\n",
    "    string_input = values_test.loc[(slice(None), ind[0]), :]\n",
    "    preds = model.predict(\n",
    "        getData(val.loc[ind], string_input, e.feat_x, e.strings_x))\n",
    "    preds = [np.argmax(x, axis=-1) for x in preds]\n",
    "    #predicted string\n",
    "    print(\"Predicted String Q\", string_input[e.strings_x[0]][0], \"from\",\n",
    "          \"-\".join(str(x) for x in string_input.index.values[0]))\n",
    "\n",
    "    if (np.argmax(string_input[e.strings_y[0]][0], axis=-1) == preds[0]).all():\n",
    "        if printCorrect: print(colors.ok + '✅' + colors.close + \"Segmentation\")\n",
    "    else:\n",
    "        print(colors.fail + '❌' + colors.close + \"Segmentation\")\n",
    "        isAllCorrect = False\n",
    "        print(\"    T\", utf2bw(string_input[e.strings_y[0]][0]))\n",
    "        print(\"     \", utf2bw(ctable.decode(preds[0][0], calc_argmax=False)))\n",
    "\n",
    "\n",
    "#     print('Q', utf2bw(pretty_join(rowx)))\n",
    "\n",
    "    rowy = dict()\n",
    "    for i, v in enumerate(e.feat_y):\n",
    "        rowy[v] = {\"correct\": val[v].loc[ind]}\n",
    "        res = np.zeros((SENTLEN, rowy[v][\"correct\"].shape[1]))\n",
    "        for ii, c in enumerate(preds[i + 1][0]):\n",
    "            res[ii, c] = 1\n",
    "        rowy[v][\"pred\"] = pd.DataFrame(res, columns=val[v].columns)\n",
    "        results = []\n",
    "        if (rowy[v][\"correct\"].values == rowy[v][\"pred\"].values).all():\n",
    "            if printCorrect: print(colors.ok + '✅' + colors.close + v)\n",
    "        else:\n",
    "            isAllCorrect = False\n",
    "            print(colors.fail + '❌' + colors.close + v, end=' ')\n",
    "            #             results.append(colors.fail + '☒' + colors.close)\n",
    "            results.append('T ' + pretty_join(rowy[v][\"correct\"]))\n",
    "            results.append(pretty_join(rowy[v][\"pred\"]))\n",
    "            print(' '.join(results))\n",
    "    if isAllCorrect or times < 10:\n",
    "        print(\"\")\n",
    "        inspectOne(times + 1, printCorrect)\n",
    "\n",
    "inspectOne(printCorrect=False)\n",
    "\n",
    "with open(\n",
    "        'output' + datetime.datetime.now().strftime(\".%Y.%m.%d.%H.%M.%S\") +\n",
    "        '.txt', 'w') as f:\n",
    "    f.write(cap.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n",
    "### After alignment. Accuracy is good. Can be treated as baseline. (name=baseline)\n",
    "`strings_cats_aligned_2018.06.25.15.14`\n",
    "### Comaprison between baseline and POS embeddings. (No drop, so it is the best) (name=baseline+pos_emb)\n",
    "`with_pos_embeddings_2018.06.25.17.37`\n",
    "### Comaprison between baseline with POS embeddings and subword embeddings. (name=baseline+pos_emb+subword_emb)\n",
    "`with_embeddings_2018.06.25.17.37` NOT DONE\n",
    "### Comaprison between baseline with subword embeddings. (name=baseline+pos_emb+subword_emb+word2vec)\n",
    "`with_word2vec_2018.06.25.17.37` NOT DONE\n",
    "\n",
    "### Different Sizes of Baseline or subword"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf1.4py3.5",
   "language": "python",
   "name": "tf1.4py3.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  },
  "notify_time": "10"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Change Log (v.4.5)\n",
    "- More clear explantation\n",
    "- sentence based data (5 dimensions: sentid, wid, mid, char, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-14T14:58:49.778614Z",
     "start_time": "2018-09-14T14:58:49.762782Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-14T14:58:52.225868Z",
     "start_time": "2018-09-14T14:58:50.724643Z"
    },
    "cell_style": "center",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "format": "column",
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "# -*- coding: utf-8 -*-\n",
    "'''\n",
    "    An implementation of sequence to sequence learning\n",
    "    for performing ensemble morphosyntactic analyses\n",
    "'''\n",
    "from __future__ import print_function\n",
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "from six.moves import range\n",
    "from prepare_data import SawarefData, padIndexes\n",
    "from character_table import colors, CharacterTable, eprint\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import re\n",
    "from ws_client import WebSocketClient\n",
    "import pickle\n",
    "import sys\n",
    "import datetime\n",
    "from keras.callbacks import TensorBoard\n",
    "from buckwalter import utf2bw, bw2utf\n",
    "from pprint import pprint\n",
    "\n",
    "# do not import in interactive mode\n",
    "# from vis import SawarefVis\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras import layers\n",
    "from keras.callbacks import EarlyStopping, Callback\n",
    "from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-23T15:55:05.579783Z",
     "start_time": "2018-07-23T15:55:05.576067Z"
    },
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Experiemnts Names\n",
    "### Compartative: \n",
    "- Morpheme-based (ru, sp1,),  `comp_ru`, `comp_sp1`, \n",
    "- character-based, `comp_ch`\n",
    "- end-to-end `comp_end`, \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Class and Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "`e.feat_x` is the input categorical features\n",
    "\n",
    "`e.feat_y` is the output categorical features\n",
    "\n",
    "`e.strings_x` is the input character-based strings features\n",
    "\n",
    "`e.strings_y` is the output character-based strings features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-14T15:00:29.261430Z",
     "start_time": "2018-09-14T15:00:29.243897Z"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Experiment:\n",
    "    feat_x = [\n",
    "        \"MXpos\", \"STpos\", \"AMpos\", \"FApos\", \n",
    "        \"STaspect\", \"AMaspect\", \"MXaspect\", \"FAaspect\", \n",
    "        \"STperson\", \"AMperson\", \"MXperson\", \"FAperson\", \n",
    "        \"STgender\", \"AMgender\", \"MXgender\", \"FAgender\", \n",
    "        \"STnumber\", \"AMnumber\", \"MXnumber\", \"FAnumber\", \n",
    "        \"STcase\", \"AMcase\", \"MXcase\", \"FAcase\", \n",
    "        \"STvoice\", \"AMvoice\", \"MXvoice\", \"FAvoice\", \n",
    "        \"STmood\", \"AMmood\", \"MXmood\", \"FAmood\", \n",
    "        \"STstate\", \"AMstate\", \"MXstate\", \"FAstate\"\n",
    "    ]\n",
    "    feat_y = [\n",
    "        \"QApos\", \"QAaspect\", \"QAperson\", \"QAgender\", \"QAnumber\", \"QAcase\",\n",
    "        \"QAvoice\", \"QAmood\", \"QAstate\"\n",
    "    ]\n",
    "    strings_x = ['in_letter','in_diac']\n",
    "    strings_y = ['out_letter','out_diac','seg']\n",
    "    instrings_x = [\"STwutf8\"] # This is the source of string_x later on\n",
    "    instrings_y = [\"QAutf8\"] # This is the source of string_x later on\n",
    "    MYPATH = \"/morpho/output/\"\n",
    "    # Parameters for the model and dataset.\n",
    "    TRAINING_SIZE = 50000\n",
    "    EPOCHS = 100\n",
    "    EMBEDDINGS = 0\n",
    "    # DIGITS = 3\n",
    "    # REVERSE = True\n",
    "    # Try replacing GRU, or SimpleRNN.\n",
    "    HIDDEN_SIZE = 128\n",
    "    BATCH_SIZE = 64\n",
    "    LAYERS = 1\n",
    "    ITERATIONS = 10\n",
    "    REVERSE = False\n",
    "    MODEL_NAME = \"main-seq-multiinput-multioutput-segmentation.keras\"\n",
    "    DATA_PICKLE = \"main-seq-multiinput-multioutput-segmentation.pickle\"\n",
    "    RNN = layers.LSTM\n",
    "    CATS_EMBEDDING = 1000\n",
    "    TEST_SPLIT = 1  #  means 0.1 of data is for test\n",
    "    VAL_SPLIT = 1  #  means 0.1 of data is for test\n",
    "    LOAD_FASTTEXT = False\n",
    "    CHARACTER_BASED = False\n",
    "    MORPHEME_BASED = False\n",
    "    previous_names = set()\n",
    "    SHUFFLE = True\n",
    "    \n",
    "    def __init__(self, exp_type=\"e\"):\n",
    "        self.exp_type = exp_type\n",
    "        self.set_constants()\n",
    "        self.set_name()\n",
    "        self.load_emb()\n",
    "    \n",
    "    def load_emb(self):\n",
    "        if self.LOAD_FASTTEXT:\n",
    "            self.emb = WebSocketClient(\"ws://localhost:8765/\")\n",
    "    def set_name(self):\n",
    "        self.NAME = self.exp_type+\"_\"+input(\"What is the name of this experiment? \")\n",
    "        self.thedate = datetime.datetime.now().strftime(\".%Y.%m.%d.%H.%M\")\n",
    "        print(\"\\nNAME=\", self.NAME)\n",
    "        print(\"DATE=\", self.thedate)\n",
    "        self.previous_names.add(self.NAME+\"%\"+self.thedate)\n",
    "        self.writer = pd.ExcelWriter('results_'+self.NAME+'.xlsx')\n",
    "    \n",
    "    def set_constants(self):\n",
    "        self.MYPATH = \"/morpho/output/\"\n",
    "        # Parameters for the model and dataset.\n",
    "        self.TRAINING_SIZE = 50000\n",
    "        self.EPOCHS = 40\n",
    "        self.EMBEDDINGS = 0\n",
    "        # DIGITS = 3\n",
    "        # REVERSE = True\n",
    "        # Try replacing GRU, or SimpleRNN.\n",
    "        self.HIDDEN_SIZE = 128\n",
    "        self.BATCH_SIZE = 64\n",
    "        self.LAYERS = 1\n",
    "        self.ITERATIONS = 10\n",
    "        self.MODEL_NAME = \"main-seq-multiinput-multioutput-segmentation.keras\"\n",
    "        self.DATA_PICKLE = \"main-seq-multiinput-multioutput-segmentation.pickle\"\n",
    "        self.RNN = layers.LSTM\n",
    "        if self.exp_type == \"comp_ru\" or self.exp_type == \"comp_sp1\":\n",
    "            self.CATS_EMBEDDING = 1000\n",
    "            self.TEST_SPLIT = 1  #  means 0.1 of data is for test\n",
    "            self.VAL_SPLIT = 1  #  means 0.1 of data is for test\n",
    "            self.LOAD_FASTTEXT = True\n",
    "            self.CHARACTER_BASED = False\n",
    "            self.MORPHEME_BASED = True\n",
    "            self.ALIGN_TYPE = \"ru-\" if self.exp_type == \"comp_ru\" else \"sp1-\"\n",
    "        elif self.exp_type == \"comp_ch\":\n",
    "            self.CATS_EMBEDDING = 1000\n",
    "            self.TEST_SPLIT = 1  #  means 0.1 of data is for test\n",
    "            self.VAL_SPLIT = 1  #  means 0.1 of data is for test\n",
    "            self.LOAD_FASTTEXT = False\n",
    "            self.CHARACTER_BASED = True\n",
    "            self.MORPHEME_BASED = False\n",
    "            self.ALIGN_TYPE = \"ch-\"            \n",
    "        elif self.exp_type == \"comp_ench\":\n",
    "            self.CATS_EMBEDDING = 1000\n",
    "            self.TEST_SPLIT = 0  #  means 0.1 of data is for test\n",
    "            self.VAL_SPLIT = 1  #  means 0.1 of data is for test\n",
    "            self.LOAD_FASTTEXT = False\n",
    "            self.CHARACTER_BASED = False\n",
    "            self.MORPHEME_BASED = False\n",
    "            self.ALIGN_TYPE = \"no-\"            \n",
    "        elif self.exp_type.startswith(\"comp_end\"):\n",
    "            self.CATS_EMBEDDING = 1000\n",
    "            self.TEST_SPLIT = 1  #  means 0.1 of data is for test\n",
    "            self.VAL_SPLIT = 1  #  means 0.1 of data is for test\n",
    "            self.LOAD_FASTTEXT = True\n",
    "            self.CHARACTER_BASED = False\n",
    "            self.MORPHEME_BASED = False\n",
    "            self.ALIGN_TYPE = \"no-\"\n",
    "\n",
    "            stripped = self.exp_type.lstrip(\"comp_end_\")\n",
    "            print(stripped)\n",
    "            if stripped in [\"MX\", \"ST\", \"FA\", \"AM\"]:\n",
    "                self.feat_x = [x for x in self.feat_x if x[0:2] == stripped]\n",
    "            elif len(stripped.split(\"_\")) >= 2:\n",
    "                self.feat_x = [x for x in self.feat_x if x[0:2] in stripped.split(\"_\")]\n",
    "            elif stripped == \"baseline\":\n",
    "                self.feat_x = []\n",
    "        else:\n",
    "            print(\"ERROR: Not a valid config name\")\n",
    "            raise NameError(\"ERROR: Not a valid config name\")\n",
    "        \n",
    "        if self.exp_type.startswith(\"comp_end_stateful\"):\n",
    "            self.SHUFFLE = False\n",
    "\n",
    "        if self.CHARACTER_BASED:\n",
    "            self.strings_x = [\"bw\"]\n",
    "        elif self.MORPHEME_BASED:\n",
    "            self.strings_x = [\"word\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-14T15:00:33.226543Z",
     "start_time": "2018-09-14T15:00:30.275225Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the name of this experiment? 4d\n",
      "\n",
      "NAME= comp_ench_4d\n",
      "DATE= .2018.09.14.16.00\n"
     ]
    }
   ],
   "source": [
    "e = Experiment(\"comp_ench\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-14T15:00:34.911296Z",
     "start_time": "2018-09-14T15:00:34.138084Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Model? (y/N)\n",
      "Training a new model. Old one with same name will be overwritten.\n"
     ]
    }
   ],
   "source": [
    "e.LOAD_MODEL = True if input(\"Load Model? (y/N)\") == \"y\" else False\n",
    "print(\"Loading Model\") if e.LOAD_MODEL else print(\"Training a new model. Old one with same name will be overwritten.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-14T15:00:35.742721Z",
     "start_time": "2018-09-14T15:00:35.739194Z"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    writer_all\n",
    "except NameError:\n",
    "    writer_all = pd.ExcelWriter('all_results.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-14T14:57:39.278246Z",
     "start_time": "2018-09-14T14:57:39.275446Z"
    }
   },
   "source": [
    "## Top-level variables (accuracies, writer_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-14T15:00:37.014638Z",
     "start_time": "2018-09-14T15:00:36.993186Z"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    accuracies\n",
    "except NameError:\n",
    "    accuracies = pd.DataFrame([],\n",
    "                 columns=list(map(list, zip(*[\n",
    "        ('QAaspect', 'acc'),('QAaspect', 'IOV_acc'),('QAaspect', 'OOV_acc'),\n",
    "        ('QAcase', 'acc'),('QAcase', 'IOV_acc'),('QAcase', 'OOV_acc'),\n",
    "        ('QAgender', 'acc'),('QAgender', 'IOV_acc'),('QAgender', 'OOV_acc'),\n",
    "        ('QAmood', 'acc'),('QAmood', 'IOV_acc'),('QAmood', 'OOV_acc'),\n",
    "        ('QAnumber', 'acc'),('QAnumber', 'IOV_acc'),('QAnumber', 'OOV_acc'),\n",
    "        ('QAperson', 'acc'),('QAperson', 'IOV_acc'),('QAperson', 'OOV_acc'),\n",
    "        ('QApos', 'acc'),('QApos', 'IOV_acc'),('QApos', 'OOV_acc'),\n",
    "        ('QAupos', 'acc'),('QAupos', 'IOV_acc'),('QAupos', 'OOV_acc'),                     \n",
    "        ('QAstate', 'acc'),('QAstate', 'IOV_acc'),('QAstate', 'OOV_acc'),\n",
    "        ('QAvoice', 'acc'),('QAvoice', 'IOV_acc'),('QAvoice', 'OOV_acc'),\n",
    "        ('QAutf8', 'acc'),('QAutf8', 'IOV_acc'),('QAutf8', 'OOV_acc'),\n",
    "        ('agg', 'all'),\n",
    "        ('agg', 'mf'),\n",
    "        ('agg', 'utf8')]))))\n",
    "try:\n",
    "    h_accuracies\n",
    "except NameError:\n",
    "    h_accuracies = pd.DataFrame([],\n",
    "                 columns=list(map(list, zip(*[\n",
    "        ('QAaspect', 'acc'),('QAaspect', 'IOV_acc'),('QAaspect', 'OOV_acc'),\n",
    "        ('QAcase', 'acc'),('QAcase', 'IOV_acc'),('QAcase', 'OOV_acc'),\n",
    "        ('QAgender', 'acc'),('QAgender', 'IOV_acc'),('QAgender', 'OOV_acc'),\n",
    "        ('QAmood', 'acc'),('QAmood', 'IOV_acc'),('QAmood', 'OOV_acc'),\n",
    "        ('QAnumber', 'acc'),('QAnumber', 'IOV_acc'),('QAnumber', 'OOV_acc'),\n",
    "        ('QAperson', 'acc'),('QAperson', 'IOV_acc'),('QAperson', 'OOV_acc'),\n",
    "        ('QApos', 'acc'),('QApos', 'IOV_acc'),('QApos', 'OOV_acc'),\n",
    "        ('QAupos', 'acc'),('QAupos', 'IOV_acc'),('QAupos', 'OOV_acc'),                     \n",
    "        ('QAstate', 'acc'),('QAstate', 'IOV_acc'),('QAstate', 'OOV_acc'),\n",
    "        ('QAvoice', 'acc'),('QAvoice', 'IOV_acc'),('QAvoice', 'OOV_acc'),\n",
    "        ('QAutf8', 'acc'),('QAutf8', 'IOV_acc'),('QAutf8', 'OOV_acc'),\n",
    "        ('agg', 'all'),\n",
    "        ('agg', 'mf'),\n",
    "        ('agg', 'utf8')]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-14T14:54:47.064467Z",
     "start_time": "2018-09-14T14:54:47.062019Z"
    }
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T19:53:10.261351Z",
     "start_time": "2018-09-15T19:53:10.242015Z"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def padIndexes(df, axis):\n",
    "    prev = df.index[0]\n",
    "    limit = max([df.index.get_level_values(axis) ] ) \n",
    "    sett = list(df.index)\n",
    "    for x in df.index:\n",
    "        # print(df.index)\n",
    "        if x[:-1] != prev[:-1]:\n",
    "            for p in range(prev[-1] + 1, limit + 1):\n",
    "                sett.append(prev[:-1] + (p,))\n",
    "        prev = x\n",
    "    for p in range(prev[-1] + 1, limit + 1):\n",
    "        sett.append(prev[:-1] + (p,))\n",
    "    return sett\n",
    "\n",
    "def pretty_join(arr):\n",
    "    if isinstance(arr, pd.Series):\n",
    "        arr = arr.to_frame().T\n",
    "    if isinstance(arr.columns, pd.core.index.MultiIndex):\n",
    "        return \"/\".join([\n",
    "            '+'.join([\n",
    "                x[1] for x in arr.columns[row == 1]\n",
    "                if x[1][-2:] != \"na\" and x[1][-2:] != \"_0\"\n",
    "            ]) for index, row in arr.iterrows()\n",
    "        ])\n",
    "    else:\n",
    "        return \"/\".join([\n",
    "            '+'.join([\n",
    "                x for x in arr.columns[row == 1]\n",
    "                if x[-2:] != \"na\" and x[-2:] != \"_0\"\n",
    "            ]) for index, row in arr.iterrows()\n",
    "        ])\n",
    "\n",
    "\n",
    "def pretty_value(colum_value):\n",
    "    return re.sub(\".*_\", \"\", colum_value)\n",
    "\n",
    "\n",
    "def getValuesAndReshape(df, middle_dim):\n",
    "    return df.values.reshape((df.shape[0] // middle_dim, middle_dim, -1))\n",
    "\n",
    "\n",
    "#     return df.values.reshape((df.shape[0]//middle_dim, middle_dim, df.shape[1]))\n",
    "\n",
    "\n",
    "def flattencolumns(df1, cols):\n",
    "    df = pd.concat(\n",
    "        [pd.DataFrame(df1[x].values.tolist()).add_prefix(x) for x in cols],\n",
    "        axis=1)\n",
    "    return pd.concat([df, df1.drop(cols, axis=1)], axis=1)\n",
    "\n",
    "\n",
    "def truncate(x):\n",
    "    return x[:EMBEDDINGS]\n",
    "\n",
    "\n",
    "def removeDiac(x):\n",
    "    return re.sub('[ًٌٍَُِّْ��]', '', x.replace(\"ٱ\",\"ا\"))\n",
    "\n",
    "\n",
    "def padStringWithSpaces(x):\n",
    "    return x + ' ' * (e.STRING_LENGTH - len(x))\n",
    "\n",
    "\n",
    "def joinMorphemesStrings(arr):\n",
    "    return \"+\".join([\n",
    "        str(x) for x in arr if isinstance(x, float) == False and x != -1\n",
    "        and x != \"-----\" and x != \"-\"\n",
    "    ])\n",
    "\n",
    "\n",
    "def fullprint(*args, **kwargs):\n",
    "    opt = np.get_printoptions()\n",
    "    np.set_printoptions(threshold='nan')\n",
    "    pprint(*args, **kwargs)\n",
    "    np.set_printoptions(**opt)\n",
    "    \n",
    "    \n",
    "def pretty_join2(arr, columns, v):\n",
    "    return \"/\".join([columns[row].replace(v+\"_\",\"\") for row in arr]).rstrip(\"/0\")    \n",
    "\n",
    "\n",
    "class TestCallback(Callback):\n",
    "    def __init__(self, test_data):\n",
    "        self.test_data = test_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        x, y = self.test_data\n",
    "        metrics = self.model.evaluate(x, y, batch_size=e.BATCH_SIZE, verbose=0)\n",
    "        for i,x in enumerate(self.model.metrics_names):\n",
    "            logs[\"test_\"+x]= metrics[i]\n",
    "        logs['test_acc']= np.mean([logs[\"test_\"+x] for x in self.model.metrics_names if x[-4:]=='_acc'])\n",
    "        logs['val_acc']= np.mean([logs[\"val_\"+x] for x in self.model.metrics_names if x[-4:]=='_acc'])\n",
    "        logs['train_acc']= np.mean([logs[x] for x in self.model.metrics_names if x[-4:]=='_acc'])\n",
    "        return logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Loading data from sawaref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T19:28:03.074465Z",
     "start_time": "2018-09-15T19:28:03.067089Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: same name exits. Results will be overwritten\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    results_all\n",
    "except NameError:\n",
    "    results_all = {}\n",
    "if e.NAME in results_all:\n",
    "    print(\"Warning: same name exits. Results will be overwritten\")\n",
    "else:\n",
    "    results_all[e.NAME] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T19:56:20.390750Z",
     "start_time": "2018-09-15T19:56:17.477321Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading from file: morpho.output.no-.pickle\n",
      "1098\n",
      "Vectorization...\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "from prepare_data import SawarefData\n",
    "sawarefData = SawarefData(\n",
    "    e.MYPATH,\n",
    "    e.EMBEDDINGS,\n",
    "    align_type=e.ALIGN_TYPE,\n",
    "    feat_x=e.feat_x,\n",
    "    strings_x=e.instrings_x,\n",
    "    strings_y=e.instrings_y,\n",
    "    feat_y=e.feat_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## DF dataframe (the origin of everthing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T19:59:31.244352Z",
     "start_time": "2018-09-15T19:59:30.784592Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "source = list(itertools.chain(*sawarefData.quran_sent))\n",
    "df = pd.DataFrame(\n",
    "    source,\n",
    "    columns=[\"sid\", \"aid\", \"wid\", \"mid\"] + e.feat_x + \n",
    "    e.instrings_x + e.instrings_y+\n",
    "    [\"embeddings\"] + e.feat_y)\n",
    "if e.EMBEDDINGS > 0:\n",
    "    df[\"embeddings\"] = df[\"embeddings\"].apply(truncate)\n",
    "    df = flattencolumns(df, [\"embeddings\"])\n",
    "df.set_index([\"sid\", \"aid\", \"wid\", \"mid\"], inplace=True)\n",
    "df.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Some Santizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T19:59:32.402922Z",
     "start_time": "2018-09-15T19:59:32.123081Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df.replace(\"0\", 0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. clean all padded rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T19:59:33.159381Z",
     "start_time": "2018-09-15T19:59:33.122830Z"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "strings = e.instrings_x + e.instrings_y\n",
    "\n",
    "for s in strings:\n",
    "    df.loc[pd.isna(df[s]), s] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Pad the rows according to the longest word (in # of morphemes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T19:59:34.284315Z",
     "start_time": "2018-09-15T19:59:34.069747Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Int64Index' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-195-77e05ee315a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSENTLEN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mid\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m df = df.reindex(\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mpadIndexes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     fill_value=-1).sort_index()\n",
      "\u001b[0;32m<ipython-input-168-a3af367b00ca>\u001b[0m in \u001b[0;36mpadIndexes\u001b[0;34m(df, axis)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;31m# print(df.index)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mprev\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m                 \u001b[0msett\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Int64Index' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "e.SENTLEN = max(df.index.get_level_values(\"mid\"))\n",
    "df = df.reindex(\n",
    "    padIndexes(df, 3),\n",
    "    fill_value=-1).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T19:56:24.471109Z",
     "start_time": "2018-09-15T19:56:24.224825Z"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for x in e.feat_y:\n",
    "    df.loc[df[\"QAutf8\"]==\"\",x] = -1\n",
    "for x in e.strings_y:\n",
    "    df.loc[df[\"QAutf8\"]==\"\",x] = -1\n",
    "    df.loc[df[\"QAutf8\"]==\"\",x+\"_undiac\"] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T19:58:17.705799Z",
     "start_time": "2018-09-15T19:58:17.469729Z"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## 3. Get the hot encoding of all caterogirical data (see columns attr)\n",
    "dumm = pd.get_dummies(df, columns=e.feat_x + e.feat_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T19:58:19.163450Z",
     "start_time": "2018-09-15T19:58:18.583411Z"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for x in e.feat_x:\n",
    "    dumm.drop(x+\"_-1\",axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T19:58:20.999605Z",
     "start_time": "2018-09-15T19:58:20.979105Z"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dumm.drop(\"embeddings\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T19:57:30.010661Z",
     "start_time": "2018-09-15T19:57:29.936385Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length mismatch: Expected axis has 50741 elements, new values have 50740 elements",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-183-351b4e28d11e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m dumm.index = [[x for x in range(e.EXAMPLES_LEN) for _ in range(e.SENTLEN)],\n\u001b[0;32m----> 2\u001b[0;31m               [x for _ in range(e.EXAMPLES_LEN) for x in range(e.SENTLEN)]]\n\u001b[0m",
      "\u001b[0;32m~/dev/py_env/tf1.4py3.5/lib/python3.5/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   4387\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4388\u001b[0m             \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4389\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4390\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4391\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/properties.pyx\u001b[0m in \u001b[0;36mpandas._libs.properties.AxisProperty.__set__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/dev/py_env/tf1.4py3.5/lib/python3.5/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_set_axis\u001b[0;34m(self, axis, labels)\u001b[0m\n\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clear_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/py_env/tf1.4py3.5/lib/python3.5/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mset_axis\u001b[0;34m(self, axis, new_labels)\u001b[0m\n\u001b[1;32m   3321\u001b[0m             raise ValueError(\n\u001b[1;32m   3322\u001b[0m                 \u001b[0;34m'Length mismatch: Expected axis has {old} elements, new '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3323\u001b[0;31m                 'values have {new} elements'.format(old=old_len, new=new_len))\n\u001b[0m\u001b[1;32m   3324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3325\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Length mismatch: Expected axis has 50741 elements, new values have 50740 elements"
     ]
    }
   ],
   "source": [
    "dumm.index = [[x for x in range(e.EXAMPLES_LEN) for _ in range(e.SENTLEN)],\n",
    "              [x for _ in range(e.EXAMPLES_LEN) for x in range(e.SENTLEN)]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T19:57:31.255671Z",
     "start_time": "2018-09-15T19:57:31.239762Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-184-7d9fa20696e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnew_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdumm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mnew_columns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'(_.*|[0-9]*)'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mdumm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnew_columns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdumm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdumm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/py_env/tf1.4py3.5/lib/python3.5/re.py\u001b[0m in \u001b[0;36msub\u001b[0;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0ma\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0mobject\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmust\u001b[0m \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     a replacement string to be used.\"\"\"\n\u001b[0;32m--> 182\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msubn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "## 4. Add two-level columns for easy indexing later (wid, mid)\n",
    "e.EXAMPLES_LEN = df.shape[0] // e.SENTLEN\n",
    "new_columns = []\n",
    "for x in dumm.columns:\n",
    "    new_columns.append(re.sub('(_.*|[0-9]*)', '', x))\n",
    "dumm.columns = [new_columns, dumm.columns]\n",
    "dumm.sort_index(axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T19:56:26.739516Z",
     "start_time": "2018-09-15T19:56:26.474239Z"
    },
    "cell_style": "center",
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## 5. Find possible values of each cat\n",
    "def getSet(df):\n",
    "    results = set()\n",
    "    df.apply(results.add)\n",
    "    return results\n",
    "\n",
    "\n",
    "embeddingInputSets = {i: getSet(df[i]) for i in e.feat_x}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T19:48:53.308489Z",
     "start_time": "2018-09-15T19:48:53.304043Z"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "e.feat_x = list(set(e.feat_x) - set([x for x in e.feat_x if len( embeddingInputSets[x])<=1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T19:48:54.561153Z",
     "start_time": "2018-09-15T19:48:53.865850Z"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df2 = pd.concat([df.reset_index(), dumm.reset_index()], axis=1)\n",
    "# df2.set_index([\"sid\", \"aid\", \"wid\", \"mid\"], inplace=True)\n",
    "df2.index = [[x for x in range(e.EXAMPLES_LEN) for _ in range(e.SENTLEN)],\n",
    "             [x for _ in range(e.EXAMPLES_LEN) for x in range(e.SENTLEN)]]\n",
    "\n",
    "if len(['embeddings' + str(x) for x in range(e.EMBEDDINGS)]) > 0:\n",
    "    df2.drop(['embeddings' + str(x) for x in range(e.EMBEDDINGS)], inplace=True, axis=1)\n",
    "# df2.columns = [(x,\"val\") if isinstance(x,str) else x  for x in df2.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T19:48:55.096996Z",
     "start_time": "2018-09-15T19:48:54.563496Z"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df2.columns = [[\"vals\" if isinstance(x, str) else x[0] for x in df2.columns],\n",
    "               [x if isinstance(x, str) else x[1] for x in df2.columns]]\n",
    "\n",
    "df2.sort_index(axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T19:48:55.186675Z",
     "start_time": "2018-09-15T19:48:55.137996Z"
    },
    "cell_style": "center",
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "del df\n",
    "del dumm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T19:49:45.318047Z",
     "start_time": "2018-09-15T19:49:45.246251Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df2[(\"is_fourty\",\"is_fourty\")] = df2[(\"vals\",\"sid\")].str.startswith(\"fourty\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Pre-process String Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T19:50:41.219321Z",
     "start_time": "2018-09-15T19:50:41.213719Z"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def emb_encode(x):\n",
    "    return np.zeros(e.emb.vector_size) if re.sub(\"\\\\s*\",\"\",x)==\"\" or x == -1 else e.emb[x]\n",
    "\n",
    "def convertStringToFeatX(x):\n",
    "    df2[(x,\"emb\")] = df2[(x,x+\"_undiac\")].apply(emb_encode)\n",
    "    return df2.join(pd.DataFrame(np.stack(df2[(x,\"emb\")].values),\n",
    "                         index=df2.index, \n",
    "                         columns=[[x+\"_emb\" for i in range(emb.vector_size)],\n",
    "                                  [x+\"_emb_\"+str(i) for i in range(emb.vector_size)]]))\n",
    "\n",
    "# if not e.CHARACTER_BASED and e.MORPHEME_BASED:\n",
    "#     df2 = convertStringToFeatX(\"word\")\n",
    "#     e.feat_x.append(\"word_emb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T19:53:46.940937Z",
     "start_time": "2018-09-15T19:53:38.155699Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# b. group them by morpheme and join with \"+\"\"\n",
    "df_strings = pd.DataFrame({\n",
    "    x: df2[\"vals\", x].groupby(level=[0]).apply(joinMorphemesStrings)\n",
    "    for x in strings\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T19:50:41.876556Z",
     "start_time": "2018-09-15T19:50:41.858603Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_strings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-162-eaa022cb8b44>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# c. pad joined morphemes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTRING_LENGTH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstrings\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf_strings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstrings\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdf_strings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_strings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadStringWithSpaces\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-162-eaa022cb8b44>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# c. pad joined morphemes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTRING_LENGTH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstrings\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf_strings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstrings\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdf_strings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_strings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadStringWithSpaces\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_strings' is not defined"
     ]
    }
   ],
   "source": [
    "# c. pad joined morphemes\n",
    "e.STRING_LENGTH = max([len(x) for k in strings for x in df_strings[k]])\n",
    "for s in strings:\n",
    "    df_strings[s] = df_strings[s].apply(padStringWithSpaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T19:50:46.092101Z",
     "start_time": "2018-09-15T19:50:46.072671Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_strings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-164-ae132332b303>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# d. encode them in one hot encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m charset = set(\"+\").union(\n\u001b[0;32m----> 3\u001b[0;31m     *[list(set(\"\".join(df_strings[x] + \"-\"))) for x in strings])\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mctable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCharacterTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcharset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTRING_LENGTH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m### Now we have one shape for all strings: (STRING_LENGTH, len(charset))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-164-ae132332b303>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# d. encode them in one hot encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m charset = set(\"+\").union(\n\u001b[0;32m----> 3\u001b[0;31m     *[list(set(\"\".join(df_strings[x] + \"-\"))) for x in strings])\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mctable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCharacterTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcharset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTRING_LENGTH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m### Now we have one shape for all strings: (STRING_LENGTH, len(charset))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_strings' is not defined"
     ]
    }
   ],
   "source": [
    "# d. encode them in one hot encoding\n",
    "charset = set(\"+\").union(\n",
    "    *[list(set(\"\".join(df_strings[x] + \"-\"))) for x in strings])\n",
    "ctable = CharacterTable(charset, e.STRING_LENGTH)\n",
    "### Now we have one shape for all strings: (STRING_LENGTH, len(charset))\n",
    "for x in strings:\n",
    "    df_strings[x + \"_onehot\"] = df_strings[x].apply(ctable.encode)\n",
    "df_strings['num'] = [x for x in range(len(df_strings))]\n",
    "df_strings.set_index('num', append=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# e. remove diac\n",
    "for x in strings:\n",
    "    df_strings[x+ \"_undiac\"] = df_strings[x].apply(removeDiac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "e.LOAD_FASTTEXT = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# f. encode them as dense vector using fastText\n",
    "for x in e.strings_x:\n",
    "    if e.LOAD_FASTTEXT:\n",
    "        df_strings[x + \"_emb\"] = df_strings[x+\"_undiac\"].apply(emb_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def emb_encode_char(x):\n",
    "    return ctable.encode(x)[0].argmax()\n",
    "if e.CHARACTER_BASED:\n",
    "    df2[\"bw_onehot\"] = df2[(\"bw\",\"bw\")].astype(str).apply(emb_encode_char)\n",
    "    df2.filter(regex=\"bw\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 4.1 Align Input and Output Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "wutfs = df2.loc[(slice(None),0),(\"vals\",e.strings_x[0])].tolist()\n",
    "wutfs = [x.replace(\"\\n\",\"\") for x in wutfs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "utfs = df2.replace(-1,\"\")[df2[(\"vals\",\"QAutf8\")]!=\"\"][(\"vals\",\"QAutf8\")].groupby(level=[0]).apply(\"+\".join).map(lambda x: re.sub(\"\\+*$\",\"\",x)).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import edit_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "is_letter = \"[^aFuNiKo~`۟@#\\[^]\"\n",
    "is_diacritic = \"[aFuNiKo~`۟@#\\[^]\"\n",
    "utfs_grouped = [re.findall(is_letter+is_diacritic+\"*\",utf2bw(x)) for x in utfs]\n",
    "wutfs_grouped = [re.findall(is_letter+is_diacritic+\"*\",utf2bw(x.replace(\"+\",\"\"))) for x in wutfs]\n",
    "\n",
    "utfs_grouped_letters_noPlus = [re.findall(is_letter,utf2bw(x).replace(\"+\",\"\")) for x in utfs]\n",
    "utfs_grouped_letters = [re.findall(is_letter,utf2bw(x)) for x in utfs]\n",
    "wutfs_grouped_letters = [re.findall(is_letter,utf2bw(x.replace(\"+\",\"\"))) for x in wutfs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "[i for i in utfs+wutfs if type(i)!=str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# import math\n",
    "def getLetterBased(x,utfs_grouped_letters, wutfs_grouped_letters):\n",
    "    sm = edit_distance.SequenceMatcher(utfs_grouped_letters[x],wutfs_grouped_letters[x])\n",
    "#     sm2 = edit_distance.SequenceMatcher(utfs_grouped_letters_noPlus[x],wutfs_grouped_letters[x])\n",
    "#     if sm2.ratio() == 1:\n",
    "#         return\n",
    "    a = []\n",
    "    b = []\n",
    "    c = []\n",
    "    counter = 0\n",
    "    atmp = []\n",
    "    acounter = []\n",
    "    for tag, i1, i2, j1, j2 in sm.get_opcodes():\n",
    "        if tag==\"delete\" and utfs_grouped[x][i1] == \"+\":\n",
    "            if len(c) > 0:\n",
    "                c[-1] = [counter]\n",
    "            counter+=1\n",
    "            continue\n",
    "        if tag == \"replace\":\n",
    "            if utfs_grouped[x][i1] == \"+\":\n",
    "                a.append(atmp) # means nothing\n",
    "                b.append(wutfs_grouped[x][j1])\n",
    "                c.append(acounter+[counter])                \n",
    "                atmp = []\n",
    "                acounter = []\n",
    "\n",
    "                if len(c) > 0:\n",
    "                    c[-1] = [counter]\n",
    "                counter+=1\n",
    "                continue\n",
    "            else:\n",
    "                tag = \"equal\"\n",
    "        if tag==\"equal\":\n",
    "            a.append(atmp+[utfs_grouped[x][i1]])\n",
    "            b.append(wutfs_grouped[x][j1])\n",
    "            c.append(acounter+[counter])                \n",
    "            atmp = []\n",
    "            acounter = []\n",
    "        elif tag==\"delete\":\n",
    "            atmp.append(utfs_grouped[x][i1])\n",
    "            acounter.append(counter)\n",
    "        elif tag==\"insert\":\n",
    "            a.append([]) # means nothing\n",
    "            b.append(wutfs_grouped[x][j1])\n",
    "#     for cc in c:\n",
    "#         for ccc in cc:\n",
    "#             if type(df2.loc[(x,ccc),(\"vals\",\"QApos\")])!= str:\n",
    "#                 print(\"Error at \", x, cc, df2.loc[(x,ccc),(\"vals\",\"QApos\")])\n",
    "#                 print(df2.loc[(x,slice(None)),:])\n",
    "            \n",
    "    return [[x, \"+\".join([str(df2.loc[(x,ccc),(\"vals\",\"QApos\")]) for ccc in cc]), \"+\".join(a[i]), b[i],\"\".join([ str(ccc) for ccc in cc])] for i, cc in enumerate(c) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "letter_based = [getLetterBased(x, utfs_grouped_letters, wutfs_grouped_letters) for x in range(len(utfs_grouped_letters))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "themax = max([len(x) for x in letter_based])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "letter_based2 =[]\n",
    "for i in range(len(letter_based)):\n",
    "    if letter_based[i] is not None:\n",
    "        letter_based2.append(letter_based[i] + [[i,'-1','','',''] for xx in range(len(letter_based[i]), themax)])\n",
    "#         letter_based2.append(letter_based[i])\n",
    "        letter_based2[i] = [xx +[\n",
    "                re.sub(is_diacritic+\"*\",\"\",xx[2]), \n",
    "                re.sub(is_letter,\"\",xx[2]),\n",
    "                re.sub(is_diacritic+\"*\",\"\",xx[3]), \n",
    "                re.sub(is_letter,\"\",xx[3])\n",
    "                ] for xx in letter_based2[i]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class CharacterTable2(object):\n",
    "    \"\"\"Given a set of characters:\n",
    "    + Encode them to a one hot integer representation\n",
    "    + Decode the one hot integer representation to their character output\n",
    "    + Decode a vector of probabilities to their character output\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, chars):\n",
    "        \"\"\"Initialize character table.\n",
    "        # Arguments\n",
    "            chars: Characters that can appear in the input.\n",
    "        \"\"\"\n",
    "        self.chars = sorted(set(chars))\n",
    "        self.char_indices = dict((c, i) for i, c in enumerate(self.chars))\n",
    "        self.indices_char = dict((i, c) for i, c in enumerate(self.chars))\n",
    "\n",
    "    def encode(self, C):\n",
    "        \"\"\"One hot encode given string C.\n",
    "        # Arguments\n",
    "            num_rows: Number of rows in the returned one hot encoding. This is\n",
    "                used to keep the # of rows for each data the same.\n",
    "        \"\"\"\n",
    "        x = np.zeros((len(self.chars),))\n",
    "        x[self.char_indices[C]] = 1\n",
    "        return x\n",
    "\n",
    "    def getCode(self, C):\n",
    "        return self.char_indices[C]\n",
    "\n",
    "    def decode(self, x, calc_argmax=True):\n",
    "        if calc_argmax:\n",
    "            x = x.argmax(axis=-1)\n",
    "        return self.indices_char[x]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_chstrings = pd.DataFrame(np.array(letter_based2).reshape((-1,9)), \n",
    "                            columns= [\"wid\",\"pos\",\"in\",\"out\",\"seg\",\"in_letter\",\"in_diac\",\"out_letter\",\"out_diac\"],\n",
    "                            index=[#[x for x in range(len(letter_based2)) for xx in range(themax)],\n",
    "                                   list(range(1,themax+1))*len(letter_based2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_chstrings[\"wid\"] = df_chstrings[\"wid\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_chstrings.set_index(\"wid\", append=True, inplace=True, verify_integrity=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ctable2s = {x: CharacterTable2(df_chstrings[x]) for x in e.strings_x + e.strings_y}\n",
    "### Now we have one shape for all strings: (STRING_LENGTH, len(charset))\n",
    "for x in e.strings_x + e.strings_y:\n",
    "    print(x)\n",
    "    df_chstrings[x + \"_onehot\"] = df_chstrings[x].apply(ctable2s[x].getCode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if \"is_fourty\" not in e.feat_x:\n",
    "    e.feat_x.append(\"is_fourty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df2[(\"vals\",\"sentid\")] = df2[(\"vals\",\"sid\")] + df2[(\"vals\",\"aid\")].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df2.set_index([(\"vals\",\"sentid\"),(\"vals\",\"wid\"),\"level_1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df2.values.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 6. Prepare splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "e.SHUFFLE=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if not e.SHUFFLE:\n",
    "    from collections import Counter\n",
    "\n",
    "    ids = [str(x[0])+\",\"+x[1] for x in df2.filter(regex=\"sid|aid\").values.tolist()]\n",
    "    ids_set = list(set(ids))\n",
    "    cntr = Counter(ids)\n",
    "    # cntr\n",
    "    ids\n",
    "    np.random.seed(0)\n",
    "    np.random.shuffle(ids_set)\n",
    "    total = 0\n",
    "    val_split_at = 0\n",
    "    test_split_at = 0\n",
    "\n",
    "    for x in range(len(ids_set)):\n",
    "        total += cntr[ids_set[x]]\n",
    "        if total > len(df2)* e.TEST_SPLIT / 10 + len(df2)* e.VAL_SPLIT / 10 and val_split_at == 0:\n",
    "            val_split_at = x\n",
    "        if total > len(df2)* e.TEST_SPLIT / 10 and test_split_at == 0:\n",
    "            test_split_at = x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def getMorphemeBasedIndeicies(arr):\n",
    "    return np.array([list(range(x*e.SENTLEN,x*e.SENTLEN+e.SENTLEN)) for x in arr]).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if e.SHUFFLE:\n",
    "    # 6. Shuffle (x, y) in unison\n",
    "    np.random.seed(0)\n",
    "    indices = list(range(e.EXAMPLES_LEN))\n",
    "    np.random.shuffle(indices)\n",
    "    # 7. Explicitly set apart 10% for validation data that we never train over.\n",
    "    test_split_at = int(e.EXAMPLES_LEN * e.TEST_SPLIT / 10)\n",
    "    val_split_at = int(e.EXAMPLES_LEN * e.TEST_SPLIT / 10 +\n",
    "                       e.EXAMPLES_LEN * e.VAL_SPLIT / 10)\n",
    "\n",
    "    values_test = df_strings.iloc[indices[:test_split_at]]\n",
    "    values_val = df_strings.iloc[indices[test_split_at:val_split_at]]\n",
    "    values_train = df_strings.iloc[indices[val_split_at:]]\n",
    "\n",
    "    test = df2.iloc[getMorphemeBasedIndeicies(indices[:test_split_at])]\n",
    "    val = df2.iloc[getMorphemeBasedIndeicies(indices[test_split_at:val_split_at])]\n",
    "    train = df2.iloc[getMorphemeBasedIndeicies(indices[val_split_at:])]\n",
    "else:\n",
    "    test_indecies = df2.drop(\"level_0\",axis=1).reset_index(level=0).set_index([(\"vals\",\"sid\"), (\"vals\",\"aid\")]).loc[\n",
    "        [tuple([x.split(\",\")[1],int(x.split(\",\")[0])]) for x in ids_set[:test_split_at]]\n",
    "        ,:].loc[:,[\"level_0\",\"level_1\"]]\n",
    "    val_indecies = df2.drop(\"level_0\",axis=1).reset_index(level=0).set_index([(\"vals\",\"sid\"), (\"vals\",\"aid\")]).loc[\n",
    "        [tuple([x.split(\",\")[1],int(x.split(\",\")[0])]) for x in ids_set[test_split_at:val_split_at]]\n",
    "        ,:].loc[:,[\"level_0\",\"level_1\"]]\n",
    "\n",
    "    values_test = df_chstrings.loc[[(xx+1,x) for x in list(test_indecies.loc[test_indecies[\"level_1\"]==0,\"level_0\"]) for xx in range(themax)],:]\n",
    "    values_val = df_chstrings.loc[[(xx+1,x) for x in list(val_indecies.loc[val_indecies[\"level_1\"]==0,\"level_0\"]) for xx in range(themax)],:]\n",
    "    values_train = df_chstrings.loc[~df_chstrings.index.isin(values_test.index.tolist()+values_val.index.tolist())]\n",
    "\n",
    "    \n",
    "    test = df2.loc[list(map(tuple, test_indecies.values)),:]\n",
    "    val = df2.loc[list(map(tuple, val_indecies.values)),:]\n",
    "    train = df2.loc[~df2.index.isin(list(map(tuple, np.concatenate((test_indecies.values,val_indecies.values), axis=0))))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    import pyperclip\n",
    "    pyperclip.copy(\"\\n\".join(sorted(list(set([x for x in train[(\"QAwutf\",e.strings_x[0])].tolist() if type(x)==str])))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "embeddingInputLists = {\n",
    "    i: [i + \"_\" + str(x) for x in embeddingInputSets[i]]\n",
    "    for i in embeddingInputSets\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "e.doFinal = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def getValuesAndReshape(df, middle_dim, test=\"\"):\n",
    "    return df.values.reshape((df.shape[0] // middle_dim, middle_dim, -1))\n",
    "\n",
    "# def setZero(df,zeros, from_val=-1, to_val=0.):\n",
    "# #     dumm[(dumm.isin([0,-1])).all(axis=1)] = -1.\n",
    "#     if zeros:\n",
    "#         df[(df==from_val).all(axis=1)] = to_val\n",
    "#     return df\n",
    "\n",
    "def getData(cat_source, str_source, cats_feats=[], strs_feats=[], others_feats=[], embeddings=False, zeros= False):\n",
    "    data = {\n",
    "        **{i:\n",
    "           getValuesAndReshape(cat_source[i].idxmax(axis=1).apply(lambda x: embeddingInputLists[i].index(x)),e.SENTLEN)\n",
    "               if i in embeddingInputSets and len(embeddingInputSets[i]) > 10000+e.CATS_EMBEDDING #and i.replace(\"_emb\",\"\") not in e.strings_x\n",
    "               else getValuesAndReshape(cat_source[i], e.SENTLEN,i) \n",
    "           for i in cats_feats},\n",
    "        **{i: getValuesAndReshape(cat_source[i], e.SENTLEN,i) for i in others_feats},\n",
    "        **{i:\n",
    "           np.eye(len(ctable2s[i].chars))[str_source[i+\"_onehot\"].values.reshape((-1, themax))] if zeros\n",
    "           else str_source[i+\"_onehot\"].values.reshape((-1, themax))\n",
    "           for i in strs_feats},\n",
    "#         **{i:np.stack(str_source[i+\"_onehot\"].values.reshape((-1, themax))) for i in strs_feats},\n",
    "    }\n",
    "    if embeddings:\n",
    "        for i in strs_feats:\n",
    "            if i+\"_emb\" not in data:\n",
    "                data[i+\"_emb\"] = np.stack(str_source[i+\"_emb\"].values)\n",
    "    if e.CHARACTER_BASED:\n",
    "        data[\"bw_onehot\"] = cat_source[\"bw_onehot\"].values.reshape((-1, e.SENTLEN))\n",
    "    if zeros:\n",
    "        for i in cats_feats:\n",
    "            data[i][np.where(data[i]==-1.)] = 0.\n",
    "    if e.doFinal and zeros:\n",
    "        data[\"sparseFinal\"] = np.sum(np.concatenate([\n",
    "            np.reshape(np.argmax(data[x],axis=-1)*data[x].shape[-1]**i,newshape=(-1,e.SENTLEN,1)) \n",
    "            for i,x in enumerate(cats_feats)],\n",
    "            axis=-1),axis=-1,keepdims=True)\n",
    "    return data\n",
    "\n",
    "\n",
    "data = {\n",
    "    'input': getData(train, values_train, cats_feats=e.feat_x, strs_feats=e.strings_x, others_feats=e.others_x, embeddings=e.LOAD_FASTTEXT),\n",
    "    'output': getData(train, values_train, cats_feats=e.feat_y, strs_feats=e.strings_y, zeros=True),\n",
    "    'val': (\n",
    "        getData(val, values_val, cats_feats=e.feat_x, strs_feats=e.strings_x, others_feats=e.others_x, embeddings=e.LOAD_FASTTEXT),\n",
    "        getData(val, values_val, cats_feats=e.feat_y, strs_feats=e.strings_y, zeros=True)\n",
    "    ),\n",
    "    'test': (\n",
    "        getData(test, values_test, cats_feats=e.feat_x, strs_feats=e.strings_x, others_feats=e.others_x, embeddings=e.LOAD_FASTTEXT),\n",
    "        getData(test, values_test, cats_feats=e.feat_y, strs_feats=e.strings_y, zeros=True)\n",
    "    )\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "[(x,data[\"output\"][x].shape[-1]) for x in e.feat_y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data[\"val\"] = tuple([data[\"val\"][0],data[\"val\"][1],\n",
    "                     {i: (data[\"val\"][1][i].argmax(axis=2) > 0).astype(int) for i in e.feat_y + e.strings_y if i in data[\"val\"][1]},]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "e.ENCH = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def getEmbedding(input):\n",
    "    name = input.name.split(\"_\")[0]\n",
    "    if name in e.feat_x and len(embeddingInputSets[name]) > e.CATS_EMBEDDING:\n",
    "        return layers.Reshape((e.SENTLEN, -1))(layers.Embedding(len(embeddingInputSets[name]),2, input_length=e.SENTLEN)(input))\n",
    "    else:\n",
    "        return layers.Dropout(0.01)(input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## End-To-End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if not e.LOAD_MODEL and not e.ENCH and not e.MORPHEME_BASED and not e.CHARACTER_BASED:\n",
    "    # print('Build model...')\n",
    "    outputs = []\n",
    "    inputs = []\n",
    "    # For strings\n",
    "    strings_input = layers.Input(shape=(e.STRING_LENGTH, len(charset)), name=e.strings_x[0])\n",
    "    lstm_strings_encoder = layers.Bidirectional(e.RNN(e.HIDDEN_SIZE,name=\"lstm_strings_encoder\"))(strings_input)\n",
    "\n",
    "    if e.LOAD_FASTTEXT:\n",
    "        emb_input = layers.Input(shape=(e.emb.vector_size,), name=e.strings_x[0]+\"_emb\")\n",
    "        inputs.append(emb_input)\n",
    "    if len(e.feat_x) == 0:\n",
    "        inputs.append(strings_input)\n",
    "        concatenated = layers.Concatenate()([lstm_strings_encoder,emb_input])\n",
    "    else:\n",
    "        # For categoricals\n",
    "        feat_x_cat = {name: [x for x in e.feat_x if x[:2] == name] for name in set([y[:2] for y in e.feat_x]) }\n",
    "        rnns = []\n",
    "        for i in feat_x_cat:\n",
    "            inp = []\n",
    "            for j in feat_x_cat[i]:\n",
    "                inp.append(layers.Input(shape=(e.SENTLEN, data[\"input\"][j].shape[2]), name=j))\n",
    "\n",
    "            # main_input = layers.Concatenate()([layers.Dropout(0.01)(input) for input in inputs])\n",
    "            input_list = [getEmbedding(input) for input in inp]\n",
    "\n",
    "            main_input = layers.Concatenate()(input_list)\n",
    "            lstm_out = layers.Bidirectional(e.RNN(e.HIDDEN_SIZE), name=\"RNN_\"+i)(main_input)\n",
    "            rnns.append(lstm_out)\n",
    "            inputs +=inp\n",
    "\n",
    "        inputs.append(strings_input)\n",
    "\n",
    "        # input_shape=(None, len(ctable_x.chars) + EMBEDDINGS)))\n",
    "        # As the decoder e.RNN's input, repeatedly provide with the last hidden state of\n",
    "        # e.RNN for each time step. Repeat 'DIGITS + 1' times as that's the maximum\n",
    "        # length of output, e.g., when DIGITS=3, max output is 999+999=1998.\n",
    "        if e.LOAD_FASTTEXT:\n",
    "            concatenated = layers.Concatenate()(rnns+[lstm_strings_encoder,emb_input])\n",
    "        else:\n",
    "            concatenated = layers.Concatenate()(rnns+[lstm_strings_encoder])\n",
    "\n",
    "    # For strings again\n",
    "    repeat_strings_out = layers.RepeatVector(e.STRING_LENGTH)(concatenated)\n",
    "    rnn_out = e.RNN(e.HIDDEN_SIZE, return_sequences=True)(repeat_strings_out)\n",
    "    strings_output = layers.TimeDistributed(\n",
    "          layers.Dense(\n",
    "            len(charset), \n",
    "            activation=\"softmax\"), name=e.strings_y[0])(rnn_out)\n",
    "    outputs.append(strings_output)\n",
    "\n",
    "    dropout_out = layers.Dropout(0.01)(concatenated)\n",
    "    repeat_out = layers.RepeatVector(e.SENTLEN)(dropout_out)\n",
    "    # The decoder e.RNN could be multiple layers stacked or a single layer.\n",
    "    rnn_out = e.RNN(e.HIDDEN_SIZE, return_sequences=True)(repeat_out)\n",
    "    for _ in range(e.LAYERS-1):\n",
    "        # By setting return_sequences to True, return not only the last output but\n",
    "        # all the outputs so far in the form of (num_samples, timesteps,\n",
    "        # output_dim). This is necessary as TimeDistributed in the below expects\n",
    "        # the first dimension to be the timesteps.\n",
    "        rnn_out = e.RNN(e.HIDDEN_SIZE, return_sequences=True)(rnn_out)\n",
    "\n",
    "\n",
    "\n",
    "    # Apply a dense layer to the every temporal slice of an input. For each of step\n",
    "    # of the output sequence, decide which character should be chosen.\n",
    "\n",
    "    for i in e.feat_y:\n",
    "        outputs.append(\n",
    "          layers.TimeDistributed(\n",
    "          layers.Dense(\n",
    "            data[\"output\"][i].shape[2], \n",
    "            activation=\"softmax\"), name=i)(rnn_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if not e.LOAD_MODEL and not e.ENCH and not e.MORPHEME_BASED and not e.CHARACTER_BASED:\n",
    "    # print('Build model...')\n",
    "    outputs = []\n",
    "    inputs = []\n",
    "    # For strings\n",
    "    strings_input = layers.Input(shape=(e.STRING_LENGTH, len(charset)), name=e.strings_x[0])\n",
    "    lstm_strings_encoder = layers.Bidirectional(e.RNN(e.HIDDEN_SIZE,name=\"lstm_strings_encoder\"))(strings_input)\n",
    "\n",
    "    if e.LOAD_FASTTEXT:\n",
    "        emb_input = layers.Input(shape=(e.emb.vector_size,), name=e.strings_x[0]+\"_emb\")\n",
    "        inputs.append(emb_input)\n",
    "    if len(e.feat_x) == 0:\n",
    "        inputs.append(strings_input)\n",
    "        concatenated = layers.Concatenate()([lstm_strings_encoder,emb_input])\n",
    "    else:\n",
    "        # For categoricals\n",
    "        feat_x_cat = {name: [x for x in e.feat_x if x[:2] == name] for name in set([y[:2] for y in e.feat_x]) }\n",
    "        rnns = []\n",
    "        for i in feat_x_cat:\n",
    "            inp = []\n",
    "            for j in feat_x_cat[i]:\n",
    "                inp.append(layers.Input(shape=(e.SENTLEN, data[\"input\"][j].shape[2]), name=j))\n",
    "\n",
    "            # main_input = layers.Concatenate()([layers.Dropout(0.01)(input) for input in inputs])\n",
    "            input_list = [getEmbedding(input) for input in inp]\n",
    "\n",
    "            main_input = layers.Concatenate()(input_list)\n",
    "            lstm_out = layers.Bidirectional(e.RNN(e.HIDDEN_SIZE), name=\"RNN_\"+i)(main_input)\n",
    "            rnns.append(lstm_out)\n",
    "            inputs +=inp\n",
    "\n",
    "        inputs.append(strings_input)\n",
    "\n",
    "        # input_shape=(None, len(ctable_x.chars) + EMBEDDINGS)))\n",
    "        # As the decoder e.RNN's input, repeatedly provide with the last hidden state of\n",
    "        # e.RNN for each time step. Repeat 'DIGITS + 1' times as that's the maximum\n",
    "        # length of output, e.g., when DIGITS=3, max output is 999+999=1998.\n",
    "        if e.LOAD_FASTTEXT:\n",
    "            concatenated = layers.Concatenate()(rnns+[lstm_strings_encoder,emb_input])\n",
    "        else:\n",
    "            concatenated = layers.Concatenate()(rnns+[lstm_strings_encoder])\n",
    "\n",
    "    # For strings again\n",
    "    repeat_strings_out = layers.RepeatVector(e.STRING_LENGTH)(concatenated)\n",
    "    rnn_out = e.RNN(e.HIDDEN_SIZE, return_sequences=True)(repeat_strings_out)\n",
    "    strings_output = layers.TimeDistributed(\n",
    "          layers.Dense(\n",
    "            len(charset), \n",
    "            activation=\"softmax\"), name=e.strings_y[0])(rnn_out)\n",
    "    outputs.append(strings_output)\n",
    "\n",
    "    dropout_out = layers.Dropout(0.01)(concatenated)\n",
    "    repeat_out = layers.RepeatVector(e.SENTLEN)(dropout_out)\n",
    "\n",
    "    # Apply a dense layer to the every temporal slice of an input. For each of step\n",
    "    # of the output sequence, decide which character should be chosen.\n",
    "\n",
    "    for i in e.feat_y:\n",
    "        # The decoder e.RNN could be multiple layers stacked or a single layer.\n",
    "        rnn_out = e.RNN(e.HIDDEN_SIZE, return_sequences=True)(repeat_out)\n",
    "        for _ in range(e.LAYERS-1):\n",
    "            # By setting return_sequences to True, return not only the last output but\n",
    "            # all the outputs so far in the form of (num_samples, timesteps,\n",
    "            # output_dim). This is necessary as TimeDistributed in the below expects\n",
    "            # the first dimension to be the timesteps.\n",
    "            rnn_out = e.RNN(e.HIDDEN_SIZE, return_sequences=True)(rnn_out)\n",
    "        outputs.append(\n",
    "          layers.TimeDistributed(\n",
    "          layers.Dense(\n",
    "            data[\"output\"][i].shape[2], \n",
    "            activation=\"softmax\"), name=i)(rnn_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-22T12:53:33.291031Z",
     "start_time": "2018-07-22T12:53:33.287984Z"
    },
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 8.2 Morpheme-Based  or 8.3 Character-Based "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if not e.LOAD_MODEL and not e.ENCH and e.MORPHEME_BASED or e.CHARACTER_BASED:\n",
    "    # print('Build model...')\n",
    "    outputs = []\n",
    "    inputs = []\n",
    "\n",
    "    # For categoricals\n",
    "    for i in e.feat_x:\n",
    "        inputs.append(layers.Input(shape=(e.SENTLEN, data[\"input\"][i].shape[2]), name=i))\n",
    "\n",
    "    def getEmbeddingWithMasking(input):\n",
    "        name = input.name.split(\"_\")[0]\n",
    "        if name in e.feat_x and len(embeddingInputSets[name]) > e.CATS_EMBEDDING:\n",
    "            return layers.Masking(mask_value=0.)(layers.Reshape((e.SENTLEN, -1))(layers.Embedding(len(embeddingInputSets[name]),2, input_length=SENTLEN)(input)))\n",
    "        else:\n",
    "            return layers.Dropout(0.1)(layers.Masking(mask_value=0.)(input))\n",
    "\n",
    "    main_input = layers.Concatenate()([getEmbedding(input) for input in inputs])\n",
    "\n",
    "    lstm_out = layers.Bidirectional(e.RNN(e.HIDDEN_SIZE, return_sequences=True))(main_input)\n",
    "    if e.LOAD_FASTTEXT and e.MORPHEME_BASED:\n",
    "        emb_input = layers.Input(shape=(e.emb.vector_size,), name=e.strings_x[0]+\"_emb\")\n",
    "        inputs.append(emb_input)\n",
    "        rnn_out = layers.Concatenate()([lstm_out,layers.RepeatVector(e.SENTLEN)(emb_input)])\n",
    "    elif e.CHARACTER_BASED:\n",
    "        emb_input = layers.Input(shape=(e.SENTLEN,), name=\"bw_onehot\")\n",
    "        inputs.append(emb_input)\n",
    "        rnn_out = layers.Concatenate()([lstm_out,layers.Embedding(len(ctable.chars),10)(emb_input)])\n",
    "    else:\n",
    "        rnn_out = lstm_out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Apply a dense layer to the every temporal slice of an input. For each of step\n",
    "    # of the output sequence, decide which character should be chosen.\n",
    "\n",
    "    for i in e.feat_y:\n",
    "        outputs.append(\n",
    "          layers.TimeDistributed(\n",
    "          layers.Dense(\n",
    "            data[\"output\"][i].shape[2], \n",
    "            activation=\"softmax\"), name=i)(rnn_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 8.3 Character End-to-End "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "e.posfeat_y = \"QApos\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "e.others_x = [\"is_fourty\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tmp = [data[\"output\"][x] for x in e.feat_y]\n",
    "# tmp = [np.array([[[0,1],[1,0]]]),np.array([[[0,0,1],[0,0,1]]])]\n",
    "# tmpo = np.array([[15,15]])\n",
    "import tensorflow as tf\n",
    "def MultipeOutputToSparseOne(x):\n",
    "    if not isinstance(x, list) or len(x) < 2:\n",
    "        raise ValueError('This layer should be called '\n",
    "                         'on a list of at least 2 inputs')\n",
    "#     print(x)\n",
    "    return K.sum(K.reshape(K.concatenate([K.reshape(            \n",
    "        K.cast(\n",
    "        K.argmax(tensor, axis=-1) * int(tensor.get_shape()[-1])**ii\n",
    "        , dtype=\"float32\")\n",
    "        , shape=(-1,x[0].get_shape()[1],1))          \n",
    "                                          for ii, tensor in enumerate(x)]\n",
    "                                                  ), \n",
    "                                     shape=(-1,x[0].get_shape()[1],len(x))),axis=-1, keepdims=True)\n",
    "\n",
    "def sparse_cross_entropy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate the cross-entropy loss between y_true and y_pred.\n",
    "    \n",
    "    y_true is a 2-rank tensor with the desired output.\n",
    "    The shape is [batch_size, sequence_length] and it\n",
    "    contains sequences of integer-tokens.\n",
    "\n",
    "    y_pred is the decoder's output which is a 3-rank tensor\n",
    "    with shape [batch_size, sequence_length, num_words]\n",
    "    so that for each sequence in the batch there is a one-hot\n",
    "    encoded array of length num_words.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate the loss. This outputs a\n",
    "    # 2-rank tensor of shape [batch_size, sequence_length]\n",
    "    tf.nn.sparse_softmax_cross_entropy_with_logits()\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_true,\n",
    "                                                          logits=y_pred)\n",
    "\n",
    "    # Keras may reduce this across the first axis (the batch)\n",
    "    # but the semantics are unclear, so to be sure we use\n",
    "    # the loss across the entire 2-rank tensor, we reduce it\n",
    "    # to a single scalar with the mean function.\n",
    "    loss_mean = tf.reduce_mean(loss)\n",
    "\n",
    "    return loss_mean\n",
    "\n",
    "#                      num_classes=sum([int(tensor.get_shape()[-1])**ii for ii, tensor in enumerate(x)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tmpinp = [layers.Input(shape=(e.SENTLEN,data[\"output\"][x].shape[-1])) for x in e.feat_y]\n",
    "# tmpinp = [layers.Input(shape=(2,2),dtype=\"int64\"),layers.Input(shape=(2,3),dtype=\"int64\")]\n",
    "# print(layers.Lambda(MultipeOutputToSparseOne)(tmpinp))\n",
    "tmpmodel = Model(inputs=tmpinp, outputs=#layers.Lambda(lambda x: K.cast(x, dtype=\"int32\"))(\n",
    "                                                          layers.Lambda(MultipeOutputToSparseOne, trainable=False, name=\"me\")(tmpinp)\n",
    "#                                                          , -1)\n",
    "                                         #            )\n",
    "                                        )\n",
    "\n",
    "# decoder_target = tf.placeholder(dtype='int32', shape=(None, e.SENTLEN))\n",
    "\n",
    "tmpmodel.compile(\n",
    "              loss=\"mean_squared_error\",#sparse_cross_entropy,#'sparse_categorical_crossentropy',\n",
    "              optimizer='RMSprop',\n",
    "              metrics=['accuracy'],\n",
    "#               target_tensors=[decoder_target]\n",
    "            )\n",
    "tmpmodel.predict(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from functools import reduce\n",
    "if e.ENCH == True:\n",
    "    # print('Build model...')\n",
    "    outputs = []\n",
    "    inputs = []\n",
    "    strings_inputs = {}\n",
    "    # For strings\n",
    "    for i in e.strings_x:\n",
    "        strings_inputs[i] = layers.Input(shape=(themax, ), name=i)\n",
    "        inputs.append(strings_inputs[i])\n",
    "    embedding_layers = [layers.Embedding(len(ctable2s[i].chars),10)(strings_inputs[i]) for i in strings_inputs]\n",
    "    \n",
    "#     lstm_strings_encoder = layers.Bidirectional(e.RNN(e.HIDDEN_SIZE,name=\"lstm_strings_encoder\"))(layers.Concatenate()(embedding_layers))\n",
    "\n",
    "    lstm_strings_encoder2 = layers.Bidirectional(e.RNN(e.HIDDEN_SIZE,name=\"lstm_strings_encoder\", return_sequences=True))(layers.Concatenate()(embedding_layers))\n",
    "\n",
    "    lstm_strings_encoder = layers.Lambda(lambda x: x[:, -1, :],name=\"LastState_returnSeqFalse\")(lstm_strings_encoder2) # get the last state only\n",
    "\n",
    "\n",
    "#     if e.LOAD_FASTTEXT:\n",
    "#         emb_input = layers.Input(shape=(e.emb.vector_size,), name=e.strings_x[0]+\"_emb\")\n",
    "#         inputs.append(emb_input)\n",
    "    if len(e.feat_x) == 0:\n",
    "#         inputs.append(strings_input)\n",
    "        concatenated = layers.Concatenate()([lstm_strings_encoder,emb_input])\n",
    "    else:\n",
    "        # For categoricals\n",
    "        feat_encoders = []\n",
    "#   BLOCK1 : THIS IS makes an encoder to each GROUP: e.g. AM or gender. \n",
    "#         feat_x_cat = {name: [x for x in e.feat_x if x[:2] == name] for name in set([y[:2] for y in e.feat_x])}\n",
    "#         for i in feat_x_cat:\n",
    "#             inp = []\n",
    "#             for j in feat_x_cat[i]:\n",
    "#                 inp.append(layers.Input(shape=(e.SENTLEN, data[\"input\"][j].shape[2]), name=j))\n",
    "\n",
    "#             # main_input = layers.Concatenate()([layers.Dropout(0.01)(input) for input in inputs])\n",
    "#             input_list = [getEmbedding(input) for input in inp]\n",
    "\n",
    "#             main_input = layers.Concatenate(name=\"Conc_\"+i)(input_list)\n",
    "#             lstm_out = layers.Bidirectional(e.RNN(e.HIDDEN_SIZE), name=\"RNN_\"+i)(main_input)\n",
    "#             feat_encoders.append(lstm_out)\n",
    "#             inputs +=inp\n",
    "#   BLOCK1 : ENDS HERE\n",
    "        inp = [layers.Input(shape=(e.SENTLEN, data[\"input\"][j].shape[2]), name=j) for j in e.feat_x]\n",
    "        inputs +=inp\n",
    "        inp = layers.Concatenate(name=\"Conc_Feat_X\")([getEmbedding(input) for input in inp])\n",
    "        feat_encoders.append(layers.Bidirectional(e.RNN(e.HIDDEN_SIZE * 4), name=\"RNN_All\")(inp))\n",
    "            \n",
    "        non_tagger_features = [layers.Input(shape=(e.SENTLEN, data[\"input\"][y].shape[2]), name=y) for y in e.others_x]\n",
    "        inputs += non_tagger_features\n",
    "        non_tagger_features = [layers.Lambda(lambda x: x[:,0,:], name=\"getFirstTimeStep\")(i) for i in non_tagger_features]\n",
    "#         inputs.append(strings_input)\n",
    "\n",
    "        # input_shape=(None, len(ctable_x.chars) + EMBEDDINGS)))\n",
    "        # As the decoder e.RNN's input, repeatedly provide with the last hidden state of\n",
    "        # e.RNN for each time step. Repeat 'DIGITS + 1' times as that's the maximum\n",
    "        # length of output, e.g., when DIGITS=3, max output is 999+999=1998.\n",
    "        if e.LOAD_FASTTEXT:\n",
    "            concatenated = layers.Concatenate(name=\"all_inputs\")(feat_encoders+[lstm_strings_encoder,emb_input] + non_tagger_features)\n",
    "        else:\n",
    "            concatenated = layers.Concatenate(name=\"all_inputs\")(feat_encoders+[lstm_strings_encoder] + non_tagger_features)\n",
    "\n",
    "    \n",
    "    # For strings again\n",
    "#     repeat_strings_out = layers.RepeatVector(e.STRING_LENGTH)(concatenated)\n",
    "#     rnn_out = e.RNN(e.HIDDEN_SIZE, return_sequences=True)(repeat_strings_out)\n",
    "    for i in e.strings_y:\n",
    "        strings_output = layers.TimeDistributed(\n",
    "              layers.Dense(\n",
    "                len(ctable2s[i].chars), \n",
    "                activation=\"softmax\"), name=i)(lstm_strings_encoder2)\n",
    "        outputs.append(strings_output)\n",
    "\n",
    "    dropout_out = layers.Dropout(0.01)(concatenated)\n",
    "    repeat_out = layers.RepeatVector(e.SENTLEN)(dropout_out)\n",
    "    # The decoder e.RNN could be multiple layers stacked or a single layer.\n",
    "    rnn_out = e.RNN(e.HIDDEN_SIZE, return_sequences=True)(repeat_out)\n",
    "    for _ in range(e.LAYERS-1):\n",
    "        # By setting return_sequences to True, return not only the last output but\n",
    "        # all the outputs so far in the form of (num_samples, timesteps,\n",
    "        # output_dim). This is necessary as TimeDistributed in the below expects\n",
    "        # the first dimension to be the timesteps.\n",
    "        rnn_out = e.RNN(e.HIDDEN_SIZE, return_sequences=True)(rnn_out)\n",
    "        \n",
    "    # Apply a dense layer to the every temporal slice of an input. For each of step\n",
    "    # of the output sequence, decide which character should be chosen.\n",
    "    \n",
    "    final_concat = []\n",
    "    oo= layers.TimeDistributed(\n",
    "      layers.Dense(\n",
    "        data[\"output\"][e.posfeat_y].shape[2], \n",
    "        activation=\"softmax\"), name=e.posfeat_y)(rnn_out)\n",
    "    final_concat.append(oo)\n",
    "    outputs.append(oo)\n",
    "    \n",
    "    for i in e.feat_y:\n",
    "        if i == e.posfeat_y:\n",
    "            continue\n",
    "        o= layers.TimeDistributed(\n",
    "          layers.Dense(\n",
    "            data[\"output\"][i].shape[2], \n",
    "            activation=\"softmax\"), name=i)(layers.Concatenate()([rnn_out,oo]))\n",
    "        final_concat.append(o)\n",
    "        outputs.append(o)\n",
    "    outputs.append(layers.Lambda(MultipeOutputToSparseOne, name=\"sparseFinal\")(final_concat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# THIS IS STATEFUL NETWORK.\n",
    "# e.BATCH_SIZE = 1\n",
    "# from keras import backend as K\n",
    "# from functools import reduce\n",
    "# if e.ENCH == True:\n",
    "#     # print('Build model...')\n",
    "#     outputs = []\n",
    "#     inputs = []\n",
    "#     strings_inputs = {}\n",
    "#     # For strings\n",
    "#     for i in e.strings_x:\n",
    "#         strings_inputs[i] = layers.Input(batch_shape=(e.BATCH_SIZE,themax, ), name=i)\n",
    "#         inputs.append(strings_inputs[i])\n",
    "#     embedding_layers = [layers.Embedding(len(ctable2s[i].chars),10)(strings_inputs[i]) for i in strings_inputs]\n",
    "    \n",
    "# #     lstm_strings_encoder = layers.Bidirectional(e.RNN(e.HIDDEN_SIZE,name=\"lstm_strings_encoder\"))(layers.Concatenate()(embedding_layers))\n",
    "\n",
    "#     lstm_strings_encoder2 = layers.Bidirectional(e.RNN(\n",
    "#         e.HIDDEN_SIZE,name=\"lstm_strings_encoder\", \n",
    "#         return_sequences=True,\n",
    "#         stateful=True))(layers.Concatenate()(embedding_layers))\n",
    "\n",
    "#     lstm_strings_encoder = layers.Lambda(lambda x: x[:, -1, :])(lstm_strings_encoder2) # get the last state only\n",
    "\n",
    "\n",
    "# #     if e.LOAD_FASTTEXT:\n",
    "# #         emb_input = layers.Input(shape=(e.emb.vector_size,), name=e.strings_x[0]+\"_emb\")\n",
    "# #         inputs.append(emb_input)\n",
    "#     if len(e.feat_x) == 0:\n",
    "# #         inputs.append(strings_input)\n",
    "#         concatenated = layers.Concatenate()([lstm_strings_encoder,emb_input])\n",
    "#     else:\n",
    "#         # For categoricals\n",
    "#         feat_x_cat = {name: [x for x in e.feat_x if x[:2] == name] for name in set([y[:2] for y in e.feat_x]) if name != \"is\"}\n",
    "#         rnns = []\n",
    "#         for i in feat_x_cat:\n",
    "#             inp = []\n",
    "#             for j in feat_x_cat[i]:\n",
    "#                 inp.append(layers.Input(batch_shape=(e.BATCH_SIZE, e.SENTLEN, data[\"input\"][j].shape[2]), name=j))\n",
    "\n",
    "#             # main_input = layers.Concatenate()([layers.Dropout(0.01)(input) for input in inputs])\n",
    "#             input_list = [getEmbedding(input) for input in inp]\n",
    "\n",
    "#             main_input = layers.Concatenate(name=\"Conc_\"+i)(input_list)\n",
    "#             lstm_out = layers.Bidirectional(e.RNN(e.HIDDEN_SIZE,stateful=True), name=\"RNN_\"+i)(main_input)\n",
    "#             rnns.append(lstm_out)\n",
    "#             inputs +=inp\n",
    "            \n",
    "#         non_tagger_features = [layers.Input(batch_shape=(e.BATCH_SIZE,e.SENTLEN, data[\"input\"][y].shape[2]), name=y) for y in e.feat_x if y[:2] == \"is\"]\n",
    "#         inputs += non_tagger_features\n",
    "#         non_tagger_features = [layers.Lambda(lambda x: x[:,0,:])(i) for i in non_tagger_features] #output_shape=(1,) + input_shape[2:])]\n",
    "# #         inputs.append(strings_input)\n",
    "\n",
    "#         # input_shape=(None, len(ctable_x.chars) + EMBEDDINGS)))\n",
    "#         # As the decoder e.RNN's input, repeatedly provide with the last hidden state of\n",
    "#         # e.RNN for each time step. Repeat 'DIGITS + 1' times as that's the maximum\n",
    "#         # length of output, e.g., when DIGITS=3, max output is 999+999=1998.\n",
    "#         if e.LOAD_FASTTEXT:\n",
    "#             concatenated = layers.Concatenate(name=\"all_inputs\")(rnns+[lstm_strings_encoder,emb_input] + non_tagger_features)\n",
    "#         else:\n",
    "#             concatenated = layers.Concatenate(name=\"all_inputs\")(rnns+[lstm_strings_encoder] + non_tagger_features)\n",
    "\n",
    "    \n",
    "#     # For strings again\n",
    "# #     repeat_strings_out = layers.RepeatVector(e.STRING_LENGTH)(concatenated)\n",
    "# #     rnn_out = e.RNN(e.HIDDEN_SIZE, return_sequences=True)(repeat_strings_out)\n",
    "#     for i in e.strings_y:\n",
    "#         strings_output = layers.TimeDistributed(\n",
    "#               layers.Dense(\n",
    "#                 len(ctable2s[i].chars), \n",
    "#                 activation=\"softmax\"), name=i)(lstm_strings_encoder2)\n",
    "#         outputs.append(strings_output)\n",
    "\n",
    "#     dropout_out = layers.Dropout(0.01)(concatenated)\n",
    "#     repeat_out = layers.RepeatVector(e.SENTLEN)(dropout_out)\n",
    "#     # The decoder e.RNN could be multiple layers stacked or a single layer.\n",
    "#     rnn_out = e.RNN(e.HIDDEN_SIZE, return_sequences=True,stateful=True)(repeat_out)\n",
    "#     for _ in range(e.LAYERS-1):\n",
    "#         # By setting return_sequences to True, return not only the last output but\n",
    "#         # all the outputs so far in the form of (num_samples, timesteps,\n",
    "#         # output_dim). This is necessary as TimeDistributed in the below expects\n",
    "#         # the first dimension to be the timesteps.\n",
    "#         rnn_out = e.RNN(e.HIDDEN_SIZE, return_sequences=True)(rnn_out)\n",
    "        \n",
    "#     # Apply a dense layer to the every temporal slice of an input. For each of step\n",
    "#     # of the output sequence, decide which character should be chosen.\n",
    "    \n",
    "#     oo= layers.TimeDistributed(\n",
    "#       layers.Dense(\n",
    "#         data[\"output\"][e.posfeat_y].shape[2], \n",
    "#         activation=\"softmax\"), name=e.posfeat_y)(rnn_out)\n",
    "#     outputs.append(oo)\n",
    "    \n",
    "#     for i in e.feat_y:\n",
    "#         if i == e.posfeat_y:\n",
    "#             continue\n",
    "#         o= layers.TimeDistributed(\n",
    "#           layers.Dense(\n",
    "#             data[\"output\"][i].shape[2], \n",
    "#             activation=\"softmax\"), name=i)(layers.Concatenate()([rnn_out,oo]))\n",
    "#         outputs.append(o)\n",
    "# final = layers.Lambda(MultipeOutputToSparseOne)(final_concat)\n",
    "# final                         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if not e.LOAD_MODEL:\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    model.compile(sample_weight_mode=\"temporal\", #if not e.MORPHEME_BASED and not e.CHARACTER_BASED else None,\n",
    "                  loss={x:'categorical_crossentropy' if x!=\"sparseFinal\" else 'mean_squared_error' for x in model.output_names},\n",
    "#                   loss_weights={x: 1.0 if x in [\"QAutf8\",\"QApos\"] else 0.2 for x in model.output_names},\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy']\n",
    "                )\n",
    "else:\n",
    "    from os.path import exists\n",
    "    from os import listdir\n",
    "    print(\"models/\" + e.NAME + \"_{}.keras\".format(e.thedate))\n",
    "    if exists(\"models/\" + e.NAME + \"_{}.keras\".format(e.thedate)):\n",
    "        model = load_model(\"models/\" + e.NAME + \"_{}.keras\".format(e.thedate))\n",
    "    else:\n",
    "        print(\"possible models: (please change the date to reload model)\")\n",
    "        print([f for f in listdir(\"models/\") if f.startswith(e.NAME)])\n",
    "        d = input(\"the new date? (empty to keep the original one)\")\n",
    "        if d != \"\":\n",
    "            e.thedate = \".\"+d\n",
    "            print(\"date has been changed. Please re-run this code\")\n",
    "        else:\n",
    "            print(\"date has not been changed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 9. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "format": "column"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "if not e.LOAD_MODEL:\n",
    "    plot_model(model, to_file='plots/model_{}.png'.format(e.NAME), )#show_shapes=True)\n",
    "SVG(model_to_dot(model, show_shapes=True, show_layer_names=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 10. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# optional\n",
    "e.set_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if not e.LOAD_MODEL:\n",
    "    earlyStopping=EarlyStopping(monitor='val_loss', patience=5, verbose=0, mode='auto')\n",
    "\n",
    "    tensorboard = TensorBoard(\n",
    "        log_dir=\"logs/\" + e.NAME + \"_{}\".format(e.thedate))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "max_len = 20\n",
    "class ResetStatesCallback(Callback):\n",
    "    def __init__(self):\n",
    "        self.counter = 0\n",
    "        self.wid_endings = [x[0] for x in list(train[train[(\"vals\",e.posfeat_y)]==0].index)]\n",
    "\n",
    "    def on_batch_begin(self, batch, logs={}):\n",
    "        if self.counter in self.wid_endings:\n",
    "            self.model.reset_states()\n",
    "        self.counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(e.NAME + \"_{}\".format(e.thedate))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not e.LOAD_MODEL:\n",
    "    history = model.fit(data['input'], data['output'],\n",
    "                    batch_size=e.BATCH_SIZE,\n",
    "                    shuffle=False,\n",
    "                    callbacks=[\n",
    "#                         ResetStatesCallback(), \n",
    "                        earlyStopping, \n",
    "#                                TestCallback(data['test']), \n",
    "                               tensorboard],\n",
    "                    epochs=e.EPOCHS,\n",
    "                    verbose=2,\n",
    "                    sample_weight={i:((data[\"output\"][i].argmax(axis=2) > 0).astype(int)*.9)+.1 for i in model.output_names},\n",
    "                    validation_data=data['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if not e.LOAD_MODEL:\n",
    "    model.save(\"models/\" + e.NAME + \"_{}.keras\".format(e.thedate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 11. Evaluate Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "reduced_mapping = {\"N\": \"NOUN\",\"PN\": \"PROPN\",\"ADJ\": \"ADJ\",\"IMPN\": \"ADJ\",\"PRP\": \"PRON\",\"DEM\": \"PRON\",\"REL\": \"PRON\",\"PRON\": \"PRON\",\"LOC\": \"ADV\",\"T\": \"ADV\",\"V\": \"VERB\",\"P\": \"ADP\",\"CONJ\": \"CCONJ\",\"SUB\": \"SCONJ\",\"ACC\": \"PART\",\"AMD\": \"PART\",\"ANS\": \"PART\",\"AVR\": \"PART\",\"CAUS\": \"PART\",\"CERT\": \"PART\",\"CIRC\": \"PART\",\"COM\": \"PART\",\"COND\": \"PART\",\"EQ\": \"PART\",\"EXH\": \"PART\",\"EXL\": \"PART\",\"EXP\": \"PART\",\"FUT\": \"PART\",\"INC\": \"PART\",\"INT\": \"PART\",\"INTG\": \"PART\",\"NEG\": \"PART\",\"PREV\": \"PART\",\"PRO\": \"PART\",\"REM\": \"PART\",\"RES\": \"PART\",\"RET\": \"PART\",\"RSLT\": \"PART\",\"SUP\": \"PART\",\"SUR\": \"PART\",\"VOC\": \"PART\",\"INL\": \"PART\",\"EMPH\": \"PART\",\"IMPV\": \"PART\",\"PRP\": \"PART\",\"DET\": \"DET\",\"INTJ\": \"INTJ\",\"X\": \"X\",\"SYM\": \"SYM\",\"PUNC\": \"PUNCT\",\"NUM\": \"NUM\", \"-1\": \"-1\", \"_\": \"_\", \"-\": \"-\",\"0\":\"0\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def calc_accuracy(model,mydata,data_length, debug=True,train_vocab=[], train_input_variable=\"\"):\n",
    "    preds = [np.argmax(x, axis=-1) for x in model.predict(mydata[0])]\n",
    "    results = pd.DataFrame([], columns=[[\"inputs\"]*(len(e.strings_x)+2)+(e.strings_y+e.feat_y)*3#+[\"agg\"]*3+[\"QAupos\"]*3\n",
    "                                        ,[*e.strings_x,\"index\",\"OOV\"]\n",
    "                                        +[\"acc\"]*len(e.strings_y+e.feat_y)\n",
    "                                        +[\"pred\"]*len(e.strings_y+e.feat_y)\n",
    "                                        +[\"actu\"]*len(e.strings_y+e.feat_y)\n",
    "                                        #+[\"all\",\"utf8\",\"mf\"]\n",
    "                                        #+ [\"acc\",\"pred\",\"actu\"]\n",
    "                                       ])\n",
    "    results.sort_index(axis=1, inplace=True)\n",
    "    for ite in range(data_length):\n",
    "        df2_row = mydata[2].iloc[ite*5]\n",
    "        r = {\n",
    "            (\"inputs\",\"index\"): str(df2_row[(\"vals\",\"sid\")])+\"-\"+str(df2_row[(\"vals\",\"aid\")])+\"-\"+str(df2_row[(\"vals\",\"wid\")])+\"-\"+str(df2_row[(\"vals\",\"mid\")])\n",
    "        }\n",
    "        for i in e.strings_x:\n",
    "            r[(\"inputs\",i)] = \"\".join(ctable2s[i].decode(np.eye(len(ctable2s[i].chars))[ii], calc_argmax=True) for ii in mydata[0][i][ite])\n",
    "        \n",
    "        r[(\"inputs\",\"OOV\")] = r[(\"inputs\",train_input_variable)] not in train_vocab if train_input_variable != \"\" else False\n",
    "        for i, v in enumerate(model.output_names):\n",
    "            if v not in e.strings_y:\n",
    "                continue\n",
    "            r[(v,\"acc\")] = (np.argmax(mydata[1][v][ite], axis=-1) == preds[i][ite]).all()\n",
    "            r[(v,\"pred\")] = utf2bw(\"\".join(ctable2s[v].decode(np.eye(len(ctable2s[v].chars))[ii]) for ii in preds[i][ite])).strip(\" \") if debug else \"\"\n",
    "            r[(v,\"actu\")] = utf2bw(\"\".join(ctable2s[v].decode(ii, calc_argmax=True) for ii in mydata[1][v][ite])).strip(\" \") if debug else \"\"\n",
    "\n",
    "        for i, v in enumerate(model.output_names):\n",
    "            if v not in e.feat_y:\n",
    "                continue\n",
    "            correct, pred = np.argmax(mydata[1][v][ite], axis=-1), preds[i][ite]\n",
    "            r[(v,\"acc\")] = (correct == pred).all()\n",
    "            r[(v,\"actu\")] = pretty_join2(correct,val[v].columns,v) if debug else \"\"\n",
    "            r[(v,\"pred\")] = pretty_join2(pred,val[v].columns,v) if debug else \"\"\n",
    "#         r[(\"agg\",\"all\")] = sum([r[(v,\"acc\")] for i, v in enumerate(model.output_names)]) == len(model.output_names)\n",
    "#         r[(\"agg\",\"utf8\")] = sum([r[(v,\"acc\")] for i, v in enumerate(model.output_names) if v != \"QAutf8\" ]) == len([x for i,v in enumerate(model.output_names) if v != \"QAutf8\"])\n",
    "#         r[(\"agg\",\"mf\")] = sum([r[(v,\"acc\")] for i, v in enumerate(model.output_names)  if v != \"QAutf8\" and v != \"QApos\" ]) == len([x for i,v in enumerate(model.output_names) if v != \"QApos\"])\n",
    "        results.loc[ite] = r\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_input_variable = \"in_letter\"\n",
    "# train_vocab=set([x for x in train[(\"QAwutf\",\"QAwutf8\")].tolist() if type(x)==str])\n",
    "train_vocab = [\"\".join(ctable2s[train_input_variable].decode(np.eye(len(ctable2s[train_input_variable].chars))[ii], \n",
    "                                                             calc_argmax=True) for ii in ite) \n",
    "               for ite in data[\"input\"][train_input_variable]]\n",
    "result = calc_accuracy(model, [data[\"val\"][0],data[\"val\"][1],val], len(data[\"val\"][0][e.strings_x[0]]),debug=True,\n",
    "                       train_vocab=train_vocab,\n",
    "                       train_input_variable=train_input_variable\n",
    "                      )\n",
    "# result = calc_accuracy(model, [*data[\"test\"],test], len(data[\"test\"][0][e.strings_x[0]]),debug=True, \n",
    "#                        train_vocab=train_vocab, \n",
    "#                        train_input_variable=train_input_variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def getUPosTag(xx, reduced_mapping=None):\n",
    "    return \"/\".join([reduced_mapping[x] for x in xx.split(\"/\")]) if reduced_mapping is not None else \"\"\n",
    "\n",
    "result[(\"QAupos\",\"actu\")] = result[(\"QApos\",\"actu\")].apply(getUPosTag, args=(reduced_mapping,))\n",
    "result[(\"QAupos\",\"pred\")] = result[(\"QApos\",\"pred\")].apply(getUPosTag, args=(reduced_mapping,))\n",
    "result[(\"QAupos\",\"acc\")] = result[(\"QAupos\",\"actu\")] == result[(\"QAupos\",\"pred\")]\n",
    "\n",
    "result[(\"inputs\",\"ar\")] = result[(\"inputs\",\"in_letter\")].apply(bw2utf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def printAcc(df, regex):\n",
    "    print(\"Columns:\", end=\"\\t\")\n",
    "    print(\"\\t\".join([x[0] for x in df.filter(regex=regex).columns]))\n",
    "\n",
    "    print(\"Agg Accuracy:\", end=\"\\t\")\n",
    "    rr = df\n",
    "    print(len(rr[rr.filter(regex=regex).astype(int).sum(axis=1) == len(rr.filter(regex=regex).columns)])/len(rr))\n",
    "\n",
    "    print(\"OOV Accuracy:\", end=\"\\t\")\n",
    "    rr = df[df[\"inputs\",\"OOV\"]==True]\n",
    "    print(len(rr[rr.filter(regex=regex).astype(int).sum(axis=1) == len(rr.filter(regex=regex).columns)])/len(rr))\n",
    "\n",
    "    print(\"IOV Accuracy:\", end=\"\\t\")\n",
    "    rr = df[df[\"inputs\",\"OOV\"]==False]\n",
    "    print(len(rr[rr.filter(regex=regex).astype(int).sum(axis=1) == len(rr.filter(regex=regex).columns)])/len(rr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(\"tagging\")\n",
    "printAcc(result,\"QAaspect.*acc|QAcase.*acc|QAgender.*acc|QAmood.*acc|QAnumber.*acc|QAperson.*acc|QAstate.*acc|QAvoice.*acc|QApos.*acc\")\n",
    "print(\"tagging except gender and person\")\n",
    "printAcc(result,\"QAaspect.*acc|QAcase.*acc|QAmood.*acc|QAperson.*acc|QAstate.*acc|QAvoice.*acc|QApos.*acc\")\n",
    "print(\"tagging except gender and person and pos\")\n",
    "printAcc(result,\"QAaspect.*acc|QAcase.*acc|QAmood.*acc|QAperson.*acc|QAstate.*acc|QAvoice.*acc\")\n",
    "print(\"segmentation\")\n",
    "printAcc(result,\"out.*acc|seg.*acc\")\n",
    "print(\"taggin and segmentation\")\n",
    "printAcc(result,\"acc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def ShowWhereFalse(df,regex):\n",
    "    return df[df.filter(regex=regex).astype(int).sum(axis=1) != len(df.filter(regex=regex).columns)].filter(regex=\"acc|in\").replace(True,\"\")\n",
    "ShowWhereFalse(result,regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(list(len(result.filter(regex=\"acc\").columns) - result.filter(regex=\"acc\").astype(int).sum(axis=1)), bins='auto')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "result.replace(\"(/-1)+\", \"\", regex=True).replace(True, \"\").to_excel(e.writer,'results_test')\n",
    "e.writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    results\n",
    "except NameError:\n",
    "    results = {}\n",
    "\n",
    "if e.NAME not in results:\n",
    "    results[e.NAME] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "results[e.NAME][\"results_val\"]=result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# del accuracies\n",
    "try:\n",
    "    accuracies\n",
    "except NameError:\n",
    "    accuracies = pd.DataFrame([],\n",
    "                 columns=list(map(list, zip(*[\n",
    "        ('QAaspect', 'acc'),('QAaspect', 'IOV_acc'),('QAaspect', 'OOV_acc'),\n",
    "        ('QAcase', 'acc'),('QAcase', 'IOV_acc'),('QAcase', 'OOV_acc'),\n",
    "        ('QAgender', 'acc'),('QAgender', 'IOV_acc'),('QAgender', 'OOV_acc'),\n",
    "        ('QAmood', 'acc'),('QAmood', 'IOV_acc'),('QAmood', 'OOV_acc'),\n",
    "        ('QAnumber', 'acc'),('QAnumber', 'IOV_acc'),('QAnumber', 'OOV_acc'),\n",
    "        ('QAperson', 'acc'),('QAperson', 'IOV_acc'),('QAperson', 'OOV_acc'),\n",
    "        ('QApos', 'acc'),('QApos', 'IOV_acc'),('QApos', 'OOV_acc'),\n",
    "        ('QAupos', 'acc'),('QAupos', 'IOV_acc'),('QAupos', 'OOV_acc'),                     \n",
    "        ('QAstate', 'acc'),('QAstate', 'IOV_acc'),('QAstate', 'OOV_acc'),\n",
    "        ('QAvoice', 'acc'),('QAvoice', 'IOV_acc'),('QAvoice', 'OOV_acc'),\n",
    "        ('out_letter', 'acc'),('out_letter', 'IOV_acc'),('out_letter', 'OOV_acc'),\n",
    "        ('out_diac', 'acc'),('out_diac', 'IOV_acc'),('out_diac', 'OOV_acc'),\n",
    "        ('seg', 'acc'),('seg', 'IOV_acc'),('seg', 'OOV_acc'),\n",
    "        ('agg', 'all'),\n",
    "        ('agg', 'mf'),\n",
    "        ('agg', 'utf8')]))))\n",
    "try:\n",
    "    h_accuracies\n",
    "except NameError:\n",
    "    h_accuracies = pd.DataFrame([],\n",
    "                 columns=list(map(list, zip(*[\n",
    "        ('QAaspect', 'acc'),('QAaspect', 'IOV_acc'),('QAaspect', 'OOV_acc'),\n",
    "        ('QAcase', 'acc'),('QAcase', 'IOV_acc'),('QAcase', 'OOV_acc'),\n",
    "        ('QAgender', 'acc'),('QAgender', 'IOV_acc'),('QAgender', 'OOV_acc'),\n",
    "        ('QAmood', 'acc'),('QAmood', 'IOV_acc'),('QAmood', 'OOV_acc'),\n",
    "        ('QAnumber', 'acc'),('QAnumber', 'IOV_acc'),('QAnumber', 'OOV_acc'),\n",
    "        ('QAperson', 'acc'),('QAperson', 'IOV_acc'),('QAperson', 'OOV_acc'),\n",
    "        ('QApos', 'acc'),('QApos', 'IOV_acc'),('QApos', 'OOV_acc'),\n",
    "        ('QAupos', 'acc'),('QAupos', 'IOV_acc'),('QAupos', 'OOV_acc'),                     \n",
    "        ('QAstate', 'acc'),('QAstate', 'IOV_acc'),('QAstate', 'OOV_acc'),\n",
    "        ('QAvoice', 'acc'),('QAvoice', 'IOV_acc'),('QAvoice', 'OOV_acc'),\n",
    "#         ('QAutf8', 'acc'),('QAutf8', 'IOV_acc'),('QAutf8', 'OOV_acc'),\n",
    "        ('out_letter', 'acc'),('out_letter', 'IOV_acc'),('out_letter', 'OOV_acc'),\n",
    "        ('out_diac', 'acc'),('out_diac', 'IOV_acc'),('out_diac', 'OOV_acc'),\n",
    "        ('seg', 'acc'),('seg', 'IOV_acc'),('seg', 'OOV_acc'),\n",
    "\n",
    "        ('agg', 'all'),\n",
    "        ('agg', 'mf'),\n",
    "        ('agg', 'utf8')]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "r = {}\n",
    "for x in list(accuracies.columns):\n",
    "    if x in result.columns:\n",
    "        r[x] = np.average(result[x].values.astype(int),axis=0)\n",
    "    elif x[1] ==\"IOV_acc\":\n",
    "        r[x] = np.average(result[result[(\"inputs\",\"OOV\")]==False][(x[0],\"acc\")].values.astype(int),axis=0)\n",
    "    elif x[1] ==\"OOV_acc\":\n",
    "        r[x] = np.average(result[result[(\"inputs\",\"OOV\")]==True][(x[0],\"acc\")].values.astype(int),axis=0)\n",
    "    else:\n",
    "        print(x)\n",
    "\n",
    "\n",
    "r[('agg', 'all')] = len(result[result.filter(regex=\"acc\").astype(int).sum(axis=1) == len(result.filter(regex=\"acc\").columns)])/len(result)\n",
    "r[('agg', 'mf')] = len(result[result.filter(regex=\"QAaspect.*acc|QAcase.*acc|QAgender.*acc|QAmood.*acc|QAnumber.*acc|QAperson.*acc|QAstate.*acc|QAvoice.*acc\").astype(int).sum(axis=1) == len(result.filter(regex=\"QAaspect.*acc|QAcase.*acc|QAgender.*acc|QAmood.*acc|QAnumber.*acc|QAperson.*acc|QAstate.*acc|QAvoice.*acc\").columns)])/len(result)\n",
    "r[('agg', 'utf8')] = len(result[result.filter(regex=\"QA.*acc\").astype(int).sum(axis=1) == len(result.filter(regex=\"QA.*acc\").columns)])/len(result)\n",
    "\n",
    "accuracies.loc[e.NAME+\"_test\"] = r\n",
    "accuracies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "result_hadith = result[result[(\"inputs\",\"index\")].str.startswith(\"fourty\")]\n",
    "result_hadith.replace(\"(/-1)+\", \"\", regex=True).replace(True, \"\").to_excel(e.writer,'results_hadith_test')\n",
    "e.writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "r = {}\n",
    "for x in list(h_accuracies.columns):\n",
    "    if x in result_hadith.columns:\n",
    "        r[x] = np.average(result_hadith[x].values.astype(int),axis=0)\n",
    "    elif x[1] ==\"IOV_acc\":\n",
    "        r[x] = np.average(result_hadith[result_hadith[(\"inputs\",\"OOV\")]==False][(x[0],\"acc\")].values.astype(int),axis=0)\n",
    "    elif x[1] ==\"OOV_acc\":\n",
    "        r[x] = np.average(result_hadith[result_hadith[(\"inputs\",\"OOV\")]==True][(x[0],\"acc\")].values.astype(int),axis=0)\n",
    "    else:\n",
    "        print(x)\n",
    "\n",
    "\n",
    "h_accuracies.loc[e.NAME+\"_test\"] = r\n",
    "h_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "result = calc_accuracy(model, [data[\"val\"][0],data[\"val\"][1],val], len(data[\"val\"][0][e.strings_x[0]]),debug=True,\n",
    "                       train_vocab=set([x for x in train[(\"QAwutf\",e.strings_x[0])].tolist() if type(x)==str]), reduced_mapping=reduced_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "result.replace(\"(/-1)+\", \"\", regex=True).replace(True, \"\").to_excel(e.writer,'results_val')\n",
    "e.writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "result_hadith = result[result[(\"inputs\",\"index\")].str.startswith(\"fourty\")]\n",
    "result_hadith.replace(\"(/-1)+\", \"\", regex=True).replace(True, \"\").to_excel(e.writer,'results_hadith_val')\n",
    "e.writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "results[e.NAME][\"results_val\"]=result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "r = {x: np.average(result[x].values.astype(int),axis=0) \n",
    "     if x in result.columns else 0 for x in list(accuracies.columns)}\n",
    "\n",
    "accuracies.loc[e.NAME+\"_val\"] = r\n",
    "accuracies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "accuracies.to_excel(writer_all,'acc')\n",
    "writer_all.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-04T20:14:41.809087Z",
     "start_time": "2018-09-04T20:14:41.805942Z"
    },
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 12. Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.plot(history.history['test_loss'])\n",
    "plt.title('model overall loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val', 'test'], loc='upper right')\n",
    "plt.savefig('plots/model_overall_loss' +\n",
    "            e.NAME + \"_{}\".format(e.thedate) + '.png')\n",
    "plt.close(fig)\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# In[471]:\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history[\"train_acc\"])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.plot(history.history['test_acc'])\n",
    "plt.title('model average accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend([x +\" = \"+ str(round(history.history[x+'_acc'][-1]*100,2)) for x in ['train', 'val', 'test']], loc='lower right')\n",
    "# for i, x in enumerate(['train', 'val', 'test']):\n",
    "#     plt.annotate(str(round(history.history[x+'_acc'][-1]*100,2)),\n",
    "#                  xy=(len(history.history[x+'_acc']), history.history[x+'_acc'][-1]), \n",
    "#                  textcoords='figure pixels', \n",
    "#                  xytext=(-20,-10))\n",
    "plt.savefig('plots/model_average_accuracy' +\n",
    "            e.NAME + \"_{}\".format(e.thedate) + '.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.plot(history.history['test_loss'])\n",
    "plt.title('model overall loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val', 'test'], loc='upper right')\n",
    "plt.savefig('plots/model_overall_loss' +\n",
    "            e.NAME + \"_{}\".format(e.thedate) + '.png')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history[\"train_acc\"])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.plot(history.history['test_acc'])\n",
    "plt.title('model average accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend([x +\" = \"+ str(round(history.history[x+'_acc'][-1]*100,2)) for x in ['train', 'val', 'test']], loc='lower right')\n",
    "# for i, x in enumerate(['train', 'val', 'test']):\n",
    "#     plt.annotate(str(round(history.history[x+'_acc'][-1]*100,2)),\n",
    "#                  xy=(len(history.history[x+'_acc']), history.history[x+'_acc'][-1]), \n",
    "#                  textcoords='figure pixels', \n",
    "#                  xytext=(-20,-10))\n",
    "plt.savefig('plots/model_average_accuracy' +\n",
    "            e.NAME + \"_{}\".format(e.thedate) + '.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# VALIDATAION = True\n",
    "# prefix = \"val_\" if VALIDATAION else \"\"\n",
    "# del prefix\n",
    "legends = []\n",
    "for x in model.output_names:\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(history.history[x+\"_acc\"])\n",
    "    legends.append(\"\"+x +\" = \"+ str(round(history.history[x+'_acc'][-1]*100,2)))\n",
    "#     plt.plot(history.history[x+\"_acc\"])\n",
    "#     legends.append(\"val_\"+x)\n",
    "#     plt.plot(history.history[\"val_\" + x+\"_acc\"])\n",
    "#     legends.append(\"train_\"+x)\n",
    "plt.title('model indiviual accuracy on test dataset')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(legends, loc='lower right')\n",
    "plt.savefig('plots/accuracy_all' +\n",
    "            e.NAME + \"_{}\".format(e.thedate) + '.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "legends = []\n",
    "for x in model.output_names:\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history[\"test_\"+x+\"_loss\"])\n",
    "    legends.append(\"\"+x +\" = \"+ str(round(history.history[x+'_loss'][-1]*100,2)))\n",
    "plt.title('model individual loss on test dataset')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(legends, loc='upper right')\n",
    "plt.savefig('plots/loss_all' +\n",
    "            e.NAME + \"_{}\".format(e.thedate) + '.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 10.1 Inspect One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#%%capture --no-stderr cap\n",
    "%autoreload 2\n",
    "\n",
    "colors.ok = ''\n",
    "colors.fail = ''\n",
    "colors.close = ''\n",
    "\n",
    "\n",
    "def inspectOne(times=0, printCorrect=True):\n",
    "    isAllCorrect = True\n",
    "    if times > 10000: # to prevent infinite loops\n",
    "        return\n",
    "    ind = np.random.randint(0, len(val.index))\n",
    "    ind = (val.index[ind][0], slice(None))\n",
    "\n",
    "    string_input = values_test.loc[(slice(None), ind[0]), :]\n",
    "    preds = model.predict(\n",
    "        getData(val.loc[ind], string_input, e.feat_x, e.strings_x))\n",
    "    preds = [np.argmax(x, axis=-1) for x in preds]\n",
    "    #predicted string\n",
    "    print(\"Predicted String Q\", string_input[e.strings_x[0]][0], \"from\",\n",
    "          \"-\".join(str(x) for x in string_input.index.values[0]))\n",
    "\n",
    "    if (np.argmax(string_input[e.strings_y[0]][0], axis=-1) == preds[0]).all():\n",
    "        if printCorrect: print(colors.ok + '✅' + colors.close + \"Segmentation\")\n",
    "    else:\n",
    "        print(colors.fail + '❌' + colors.close + \"Segmentation\")\n",
    "        isAllCorrect = False\n",
    "        print(\"    T\", utf2bw(string_input[e.strings_y[0]][0]))\n",
    "        print(\"     \", utf2bw(ctable.decode(preds[0][0], calc_argmax=False)))\n",
    "\n",
    "\n",
    "#     print('Q', utf2bw(pretty_join(rowx)))\n",
    "\n",
    "    rowy = dict()\n",
    "    for i, v in enumerate(e.feat_y):\n",
    "        rowy[v] = {\"correct\": val[v].loc[ind]}\n",
    "        res = np.zeros((SENTLEN, rowy[v][\"correct\"].shape[1]))\n",
    "        for ii, c in enumerate(preds[i + 1][0]):\n",
    "            res[ii, c] = 1\n",
    "        rowy[v][\"pred\"] = pd.DataFrame(res, columns=val[v].columns)\n",
    "        results = []\n",
    "        if (rowy[v][\"correct\"].values == rowy[v][\"pred\"].values).all():\n",
    "            if printCorrect: print(colors.ok + '✅' + colors.close + v)\n",
    "        else:\n",
    "            isAllCorrect = False\n",
    "            print(colors.fail + '❌' + colors.close + v, end=' ')\n",
    "            #             results.append(colors.fail + '☒' + colors.close)\n",
    "            results.append('T ' + pretty_join(rowy[v][\"correct\"]))\n",
    "            results.append(pretty_join(rowy[v][\"pred\"]))\n",
    "            print(' '.join(results))\n",
    "    if isAllCorrect or times < 10:\n",
    "        print(\"\")\n",
    "        inspectOne(times + 1, printCorrect)\n",
    "\n",
    "inspectOne(printCorrect=False)\n",
    "\n",
    "with open(\n",
    "        'output' + datetime.datetime.now().strftime(\".%Y.%m.%d.%H.%M.%S\") +\n",
    "        '.txt', 'w') as f:\n",
    "    f.write(cap.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n",
    "### After alignment. Accuracy is good. Can be treated as baseline. (name=baseline)\n",
    "`strings_cats_aligned_2018.06.25.15.14`\n",
    "### Comaprison between baseline and POS embeddings. (No drop, so it is the best) (name=baseline+pos_emb)\n",
    "`with_pos_embeddings_2018.06.25.17.37`\n",
    "### Comaprison between baseline with POS embeddings and subword embeddings. (name=baseline+pos_emb+subword_emb)\n",
    "`with_embeddings_2018.06.25.17.37` NOT DONE\n",
    "### Comaprison between baseline with subword embeddings. (name=baseline+pos_emb+subword_emb+word2vec)\n",
    "`with_word2vec_2018.06.25.17.37` NOT DONE\n",
    "\n",
    "### Different Sizes of Baseline or subword"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf1.4py3.5",
   "language": "python",
   "name": "tf1.4py3.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  },
  "notify_time": "10",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
